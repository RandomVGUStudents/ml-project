{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     },
     "name": "setup"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/mgm19ngyq575kh4dhwm18kq138h58lnh-python3-3.13.9-env/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import marimo as mo\n",
    "\n",
    "# Miscellaneous\n",
    "import glob\n",
    "import hashlib\n",
    "import os\n",
    "import tarfile\n",
    "import typing\n",
    "import wget\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import re\n",
    "import re2\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import mailparser\n",
    "\n",
    "# Plotting\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Load NLP module\n",
    "print(spacy.prefer_gpu())\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\"label\": \"TIME\", \"pattern\": [{\"SHAPE\": \"dd:dd:dd\"}]},\n",
    "]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Hi there!\n",
    "\n",
    "This is the notebook for our Machine Learning project.\n",
    "<br>\n",
    "Let's take a look at the table of content (it is also available on the right side of the screen):\n",
    "\n",
    "I. Data Acquisition\n",
    "- Download dataset\n",
    "- Load whole dataset\n",
    "- Preprocess email\n",
    "\n",
    "II. Feature Engineering\n",
    "- Additional features to consider\n",
    "\n",
    "III. Training\n",
    "- Train time\n",
    "- Confusion time\n",
    "\n",
    "IV. Model Analysis\n",
    "\n",
    "**Let's get started!**\n",
    "\n",
    "üí° Tips: Click ‚ñ∂Ô∏è in the down right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# I. Data Acquisition\n",
    "\n",
    "In this section, we obtain the data from the Apache's SpamAssassin Public Corpus and read them.\n",
    "<br>\n",
    "After that, we can load all emails into a DataFrame.\n",
    "\n",
    "## 1. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"a-define-dataset-source-and-download-function\">a. Define dataset source and download function</h3>\n",
       "<span class=\"paragraph\">We have 5 files:</span>\n",
       "<ul>\n",
       "<li>easy_ham (2500 ham emails)</li>\n",
       "<li>easy_ham_2 (1400 ham emails)</li>\n",
       "<li>hard_ham (250 ham emails)</li>\n",
       "<li>spam (500 spam emails)</li>\n",
       "<li>spam_2 (1396 spam emails) <em>Although they said there were 1397 emails, there are only 1396 files in the extracted folder.</em></li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">Our download function will download the files and extract into <code>./datasets/</code> folder.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_source = {\n",
    "    'easy_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\",\n",
    "        'count': 2500,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'easy_ham_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\",\n",
    "        'count': 1400,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'hard_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\",\n",
    "        'count': 250,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'spam': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\",\n",
    "        'count': 500,\n",
    "        'is_spam': True\n",
    "    },\n",
    "    'spam_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\",\n",
    "        'count': 1396,\n",
    "        'is_spam': True\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_dir = \"./datasets/\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "\n",
    "def download_dataset(dataset_path: str, dataset_url: str):\n",
    "    tmp = wget.download(dataset_url)\n",
    "\n",
    "    with tarfile.open(tmp, \"r:bz2\") as tar:\n",
    "        tar.extractall(dataset_dir)\n",
    "\n",
    "    os.remove(os.path.join(dataset_path, \"cmds\"))\n",
    "    os.remove(tmp)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### a. Define dataset source and download function\n",
    "\n",
    "We have 5 files:\n",
    "\n",
    "- easy_ham (2500 ham emails)\n",
    "- easy_ham_2 (1400 ham emails)\n",
    "- hard_ham (250 ham emails)\n",
    "- spam (500 spam emails)\n",
    "- spam_2 (1396 spam emails) *Although they said there were 1397 emails, there are only 1396 files in the extracted folder.*\n",
    "\n",
    "Our download function will download the files and extract into `./datasets/` folder.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-check-dataset-integrity\">b. Check dataset integrity</h3>\n",
       "<span class=\"paragraph\">The file name of each email is its MD5 hash.\n",
       "<br>\n",
       "We can use it to verify the integrity of the dataset.</span>\n",
       "<span class=\"paragraph\"><em>Note: A file has incorrect hash from source, that's not our fault.</em></span>\n",
       "<span class=\"paragraph\">We will redownload the dataset if any file is corrupted or missing.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _dataset_name, _dataset_info in dataset_source.items():\n",
    "    _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "    if os.path.exists(_dataset_path):\n",
    "\n",
    "        if len(os.listdir(_dataset_path)) != _dataset_info['count']:\n",
    "            print(f\"Dataset {_dataset_name} is missing some files, let's redownload.\")\n",
    "            download_dataset(_dataset_name, _dataset_info['url'])\n",
    "\n",
    "        for _filename in os.listdir(_dataset_path):\n",
    "            _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "            if os.path.isfile(_file_path):\n",
    "                _md5_hash = hashlib.md5(open(_file_path, 'rb').read()).hexdigest()\n",
    "                _provided_hash = _filename.split(\".\")[1]\n",
    "\n",
    "                # Ignore wrong hash from source\n",
    "                if (_md5_hash != _provided_hash and _provided_hash != \"244a63cd74c81123ef26129453e32c95\"):\n",
    "                    print(f\"Hash mismatch for {_filename}, redownloading dataset {_dataset_name}...\")\n",
    "\n",
    "                    download_dataset(_dataset_name, _dataset_info['url'])\n",
    "                    os.remove(os.path.join(_dataset_path, _filename))\n",
    "\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        print(f\"Missing dataset {_dataset_name}, downloading it now...\")\n",
    "        download_dataset(_dataset_path, _dataset_info['url'])\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Check dataset integrity\n",
    "\n",
    "The file name of each email is its MD5 hash.\n",
    "<br>\n",
    "We can use it to verify the integrity of the dataset.\n",
    "\n",
    "*Note: A file has incorrect hash from source, that's not our fault.*\n",
    "\n",
    "We will redownload the dataset if any file is corrupted or missing.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"c-viewing-some-emails\">c. Viewing some emails</h3>\n",
       "<span class=\"paragraph\">Let's take a look at some emails that we've got.</span>\n",
       "<span class=\"paragraph\">To save us some hassle, we won't be looking as raw email files (feel free to look at them, though!)\n",
       "<br>\n",
       "However, we will be using the <code>mailparser</code> library to parse the emails.</span>\n",
       "<span class=\"paragraph\">What we care about:</span>\n",
       "<ul>\n",
       "<li>Subject</li>\n",
       "<li>Plain text content</li>\n",
       "<li>HTML content</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">üí° Tips: Click \"Fullscreen\" or \"Expand output\" button (they will appear on the right side when you hover the box below).\n",
       "<br>\n",
       "Modify the number to view different emails.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mails = glob.glob(\"./datasets/*/*\", recursive=True)\n",
    "idx = mo.ui.number(start=0, stop=len(mails) - 1, label=\"Number\")\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### c. Viewing some emails\n",
    "\n",
    "Let's take a look at some emails that we've got.\n",
    "\n",
    "To save us some hassle, we won't be looking as raw email files (feel free to look at them, though!)\n",
    "<br>\n",
    "However, we will be using the `mailparser` library to parse the emails.\n",
    "\n",
    "What we care about:\n",
    "\n",
    "- Subject\n",
    "- Plain text content\n",
    "- HTML content\n",
    "\n",
    "üí° Tips: Click \"Fullscreen\" or \"Expand output\" button (they will appear on the right side when you hover the box below).\n",
    "<br>\n",
    "Modify the number to view different emails.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><marimo-ui-element object-id='lEQa-0' random-id='08ffb2b9-24ea-5d54-0e6b-a153891b16fe'><marimo-number data-initial-value='0' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Number&lt;/span&gt;&lt;/span&gt;&quot;' data-start='0' data-stop='6045' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Path: <code>./datasets/easy_ham/00016.ef397cef16f8041242e3b6560e168053</code></span>\n",
       "<div class=\"language-text codehilite\"><pre><span></span><code>-- SUBJ --\n",
       "[IIU] Eircom aDSL Nat&#39;ing**\n",
       "-- TEXT --\n",
       "Hi all,\n",
       "\n",
       "apologies for the possible silly question (i don&#39;t think it is, but), \n",
       "but is Eircom&#39;s aDSL service NAT&#39;ed?\n",
       "\n",
       "and what implications would that have for VoIP? I know there are \n",
       "difficulties with VoIP or connecting to clients connected to a NAT&#39;ed \n",
       "network from the internet wild (i.e. machines with static, real IPs)\n",
       "\n",
       "any help pointers would be helpful,\n",
       "\n",
       "cheers\n",
       "-- \n",
       "rgrds,\n",
       "Bernard\n",
       "-- \n",
       "Bernard Tyers * National Centre for Sensor Research * P:353-1-700-5273 *\n",
       "E: bernard.tyers@dcu.ie * W: www.physics.dcu.ie/~bty * L:N117\n",
       "\n",
       "_______________________________________________\n",
       "IIU mailing list\n",
       "IIU@iiu.taint.org\n",
       "http://iiu.taint.org/mailman/listinfo/iiu\n",
       "\n",
       "\n",
       "-- HTML --\n",
       "&lt;No HTML content&gt;\n",
       "</code></pre></div></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mail = mailparser.parse_from_file(mails[idx.value])\n",
    "\n",
    "\n",
    "mo.vstack([\n",
    "    idx,\n",
    "    mo.md(f\"\"\"\n",
    "Path: `{mails[idx.value]}`\n",
    "\n",
    "```text\n",
    "-- SUBJ --\n",
    "{mail.subject}**\n",
    "-- TEXT --\n",
    "{\"---\\n\\n---\".join(mail.text_plain) if mail.text_plain else \"<No plain text content>\"}\n",
    "-- HTML --\n",
    "{mail.text_html if mail.text_html else \"<No HTML content>\"}\n",
    "```\n",
    "\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"2-load-whole-dataset\">2. Load whole dataset</h2>\n",
       "<span class=\"paragraph\">Now let's load everything into a DataFrame.</span>\n",
       "<span class=\"paragraph\">Let's understand why we need checkpoints.\n",
       "<br>\n",
       "We don't want to reprocess the dataset every time we run the notebook, so we will save the processed dataset into a file.\n",
       "<br>\n",
       "In subsequent runs, we can just load it back.</span>\n",
       "<span class=\"paragraph\">Here we have some checkpoints and some functions to save and load datasets.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_checkpoints = {\n",
    "    'orig': {\n",
    "        'description': \"Dataset without preprocessing (6046 entries)\",\n",
    "        'checksum': \"6ab4e23a696aeac72ad3b38396666a25\"\n",
    "    },\n",
    "    'pre_cleaned': {\n",
    "        'description': \"Dataset after pre-cleanup (5851 entries)\",\n",
    "        'checksum': \"589f556bc02bdeaa376ca92a99e9755c\"\n",
    "    },\n",
    "    'post_cleaned': {\n",
    "        'description': \"Dataset after post-cleanup (5851 entries)\",\n",
    "        'checksum': \"d972cc17841c34ed3f73c1bd3e3fad62\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(checkpoint: str) -> pd.DataFrame:\n",
    "    dataset_path = os.path.join(dataset_dir, f\"{checkpoint}.gzip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"No checkpoint '{checkpoint}' in {dataset_dir}\")\n",
    "\n",
    "    hash = hashlib.md5(open(dataset_path, 'rb').read()).hexdigest()\n",
    "    if hash != dataset_checkpoints[checkpoint]['checksum']:\n",
    "        print(f\"Expected: {dataset_checkpoints[checkpoint]['checksum']}, got: {hash}\")\n",
    "        raise ValueError(\"Dataset file broken.\")\n",
    "\n",
    "    data = pd.read_parquet(dataset_path)\n",
    "    return data\n",
    "\n",
    "def save_dataset(df: pd.DataFrame, checkpoint_name: str):\n",
    "    path = os.path.join(dataset_dir, f\"{checkpoint_name}.gzip\")\n",
    "    df.to_parquet(\n",
    "        path=path,\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    hash = hashlib.md5(open(path, 'rb').read()).hexdigest()\n",
    "    print(f\"File: {path}\\nMD5: {hash}\")\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "## 2. Load whole dataset\n",
    "\n",
    "Now let's load everything into a DataFrame.\n",
    "\n",
    "Let's understand why we need checkpoints.\n",
    "<br>\n",
    "We don't want to reprocess the dataset every time we run the notebook, so we will save the processed dataset into a file.\n",
    "<br>\n",
    "In subsequent runs, we can just load it back.\n",
    "\n",
    "Here we have some checkpoints and some functions to save and load datasets.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">We use <code>mailparser</code> to parse 6k+ emails into a DataFrame, and save it for later.</span>\n",
       "<span class=\"paragraph\">We will only do that when the checkpoint isn't available, or is somehow broken.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = load_dataset('orig')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    data = {\n",
    "        'subject': [],\n",
    "        'text': [],\n",
    "        'html': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for _dataset_name, _dataset_info in dataset_source.items():\n",
    "        _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "        if os.path.exists(_dataset_path):\n",
    "\n",
    "            for _filename in os.listdir(_dataset_path):\n",
    "                _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "                if os.path.isfile(_file_path):\n",
    "                    try:\n",
    "                        _mail = mailparser.parse_from_file(_file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with file {_file_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    data['subject'].append(_mail.subject)\n",
    "                    data['text'].append(\"\\n\".join(_mail.text_plain))\n",
    "                    data['html'].append(\"\\n\".join(_mail.text_html))\n",
    "                    data['label'].append(int(_dataset_info['is_spam']))\n",
    "\n",
    "    print(f\"{str(e)}\\nLoading from files...\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    save_dataset(df, 'orig')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "We use `mailparser` to parse 6k+ emails into a DataFrame, and save it for later.\n",
    "\n",
    "We will only do that when the checkpoint isn't available, or is somehow broken.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Preprocess email\n",
    "\n",
    "A little rant about how ML works in this problem:\n",
    "\n",
    "- Emails are broken into 'tokens'. Tokens are words (in simple form), or special ones.\n",
    "- The tokens are the features for the model.\n",
    "\n",
    "Here's what we need to do:\n",
    "\n",
    "1. Remove duplicates from raw dataset.\n",
    "2. Find and replace certain tokens with regex.\n",
    "3. Find and replace tokens using Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### a. Data exploration notes\n",
    "\n",
    "- High correlation: HTML usage and spam\n",
    "- Dataset artifact: a lot of spam emails have this:\n",
    "\n",
    "```text\n",
    "--DeathToSpamDeathToSpamDeathToSpam--\n",
    "```\n",
    "\n",
    "- There might be (mostly in spam emails) forms with `_____` blanks.\n",
    "- Mailing list footer should be removed, they mostly appear in ham emails.\n",
    "\n",
    "```text\n",
    "-------------------------------------------------------\n",
    "This sf.net email is sponsored by:ThinkGeek\n",
    "Welcome to geek heaven.\n",
    "http://thinkgeek.com/sf\n",
    "_______________________________________________\n",
    "Spamassassin-talk mailing list\n",
    "Spamassassin-talk@lists.sourceforge.net\n",
    "https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
    "```\n",
    "\n",
    "- ~~Email replies appear with `> ` or `--] `, and \"On DATE, USER wrote:\"~~ Nvm, it's pretty random.\n",
    "- Mailling list (or some name) in subject that is not filtered (like ilug)\n",
    "- Entity labels exclusion: \"CARDINAL\", \"ORDINAL\"\n",
    "- Some emails are malformed, they are extremely long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-remove-duplicates\">b. Remove duplicates</h3></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deduplicated_df = df.drop_duplicates(subset=['text', 'html'])\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Remove duplicates\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### c. Preprocess preview\n",
    "\n",
    "This is merely the scratchpad for whatever we want to do with these emails.\n",
    "\n",
    "Change the number to view different emails, just like above.\n",
    "<br>\n",
    "*(Also don't forget to use fullscreen or expand output)*\n",
    "\n",
    "The first column contains subject + plain text + parsed HTML (more information down below).\n",
    "<br>\n",
    "The second column shows how each token is transformed (in `nlp_pipeline` section down below).\n",
    "<br>\n",
    "The third one shows the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><marimo-ui-element object-id='lEQa-0' random-id='08ffb2b9-24ea-5d54-0e6b-a153891b16fe'><marimo-number data-initial-value='0' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Number&lt;/span&gt;&lt;/span&gt;&quot;' data-start='0' data-stop='6045' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Label: Ham</span></span><div style='display: flex;flex: 1;flex-direction: row;justify-content: space-between;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><div style='flex: 1'><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Eircom aDSL Nat'ing Hi all,  apologies for the possible silly question (i don't think it is, but),  but is Eircom's aDSL service NAT'ed?  and what implications would that have for VoIP? I know there are  difficulties with VoIP or connecting to clients connected to a NAT'ed  network from the internet wild (i.e. machines with static, real IPs)  any help pointers would be helpful,  cheers --  rgrds, Bernard --  Bernard Tyers * National Centre for Sensor Research * P:353-1-700-5273 * E: <a href=\"&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#98;&#101;&#114;&#110;&#97;&#114;&#100;&#46;&#116;&#121;&#101;&#114;&#115;&#64;&#100;&#99;&#117;&#46;&#105;&#101;\">&#98;&#101;&#114;&#110;&#97;&#114;&#100;&#46;&#116;&#121;&#101;&#114;&#115;&#64;&#100;&#99;&#117;&#46;&#105;&#101;</a> * W: <a href=\"http://www.physics.dcu.ie/~bty\" rel=\"noopener noreferrer\" target=\"_blank\">www.physics.dcu.ie/~bty</a> * L:N117  <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong>_____ IIU mailing list <a href=\"&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#73;&#73;&#85;&#64;&#105;&#105;&#117;&#46;&#116;&#97;&#105;&#110;&#116;&#46;&#111;&#114;&#103;\">&#73;&#73;&#85;&#64;&#105;&#105;&#117;&#46;&#116;&#97;&#105;&#110;&#116;&#46;&#111;&#114;&#103;</a> <a href=\"http://iiu.taint.org/mailman/listinfo/iiu\" rel=\"noopener noreferrer\" target=\"_blank\">http://iiu.taint.org/mailman/listinfo/iiu</a></span></span></div><div style='flex: 1.2'><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\"><em>org</em> <sub>&lt;-Eircom</sub> <em>propn</em> <sub>&lt;-aDSL</sub> <em>propn</em> <sub>&lt;-Nat'ing</sub>   hi <del>all</del> ,    apology <del>for</del> <del>the</del> possible silly question ( <del>i</del> <del>do</del> <del>n't</del> think <del>it</del> <del>is</del> , <del>but</del> ) ,   <del>but</del> <del>is</del> <em>org</em> <sub>&lt;-Eircom</sub> <del>'s</del> adsl service <em>propn</em> <sub>&lt;-NAT'ed</sub> ?    <del>and</del> <del>what</del> implication <del>would</del> <del>that</del> <del>have</del> <del>for</del> voip ? <del>I</del> know <del>there</del> <del>are</del>   difficulty <del>with</del> voip <del>or</del> connect <del>to</del> client connect <del>to</del> <del>a</del> <em>propn</em> <sub>&lt;-NAT'ed</sub>   network <del>from</del> <del>the</del> internet wild ( <em>foreign</em> <sub>&lt;-i.e.</sub> machine <del>with</del> static , real ip )    <del>any</del> help pointer <del>would</del> <del>be</del> helpful ,    cheers   --   <em>propn</em> <sub>&lt;-rgrds</sub> ,   <em>propn</em> <sub>&lt;-Bernard</sub>   --   <em>propn</em> <sub>&lt;-Bernard</sub> <em>propn</em> <sub>&lt;-Tyers</sub> * <em>org</em> <sub>&lt;-National</sub> <sub>Centre</sub> <sub>for</sub> <sub>Sensor</sub> <sub>Research</sub> * p:353 <em>num</em> <sub>&lt;--</sub> <em>num</em> <sub>&lt;-1</sub> <em>num</em> <sub>&lt;--</sub> <em>num</em> <sub>&lt;-700</sub> <em>num</em> <sub>&lt;--</sub> <em>num</em> <sub>&lt;-5273</sub> *   e : <em>email</em> <sub>&lt;<a href=\"&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#45;&#98;&#101;&#114;&#110;&#97;&#114;&#100;&#46;&#116;&#121;&#101;&#114;&#115;&#64;&#100;&#99;&#117;&#46;&#105;&#101;\">&#45;&#98;&#101;&#114;&#110;&#97;&#114;&#100;&#46;&#116;&#121;&#101;&#114;&#115;&#64;&#100;&#99;&#117;&#46;&#105;&#101;</a></sub> * w : <em>url</em> <sub>&lt;-<a href=\"http://www.physics.dcu.ie/~bty\" rel=\"noopener noreferrer\" target=\"_blank\">www.physics.dcu.ie/~bty</a></sub> * <em>propn</em> <sub>&lt;-L</sub> : n117    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   <em>propn</em> <sub>&lt;-IIU</sub> mailing list   <em>email</em> <sub>&lt;<a href=\"&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#45;&#73;&#73;&#85;&#64;&#105;&#105;&#117;&#46;&#116;&#97;&#105;&#110;&#116;&#46;&#111;&#114;&#103;\">&#45;&#73;&#73;&#85;&#64;&#105;&#105;&#117;&#46;&#116;&#97;&#105;&#110;&#116;&#46;&#111;&#114;&#103;</a></sub>   <em>url</em> <sub>&lt;-<a href=\"http://iiu.taint.org/mailman/listinfo/iiu\" rel=\"noopener noreferrer\" target=\"_blank\">http://iiu.taint.org/mailman/listinfo/iiu</a></sub></span></span></div><div style='flex: 0.9'><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\"><em>org</em> <em>propn</em> <em>propn</em>   hi  ,    apology   possible silly question (    think   ,  ) ,     <em>org</em>  adsl service <em>propn</em> ?      implication     voip ?  know     difficulty  voip  connect  client connect   <em>propn</em>   network   internet wild ( <em>foreign</em> machine  static , real ip )     help pointer   helpful ,    cheers   --   <em>propn</em> ,   <em>propn</em>   --   <em>propn</em> <em>propn</em> * <em>org</em> * p:353 <em>num</em> <em>num</em> <em>num</em> <em>num</em> <em>num</em> <em>num</em> *   e : <em>email</em> * w : <em>url</em> * <em>propn</em> : n117    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   <em>propn</em> mailing list   <em>email</em>   <em>url</em></span></span></div></div><div style='display: flex;flex: 1;flex-direction: row;justify-content: space-between;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><div style='flex: 1'><marimo-ui-element object-id='Hstk-1' random-id='2171a99d-e8c5-b427-0572-c088be13c653'><marimo-table data-initial-value='[]' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;TF-IDF&lt;/span&gt;&lt;/span&gt;&quot;' data-data='&quot;[{&#92;&quot;word&#92;&quot;:&#92;&quot;_org_&#92;&quot;,&#92;&quot;idx&#92;&quot;:4},{&#92;&quot;word&#92;&quot;:&#92;&quot;_propn_&#92;&quot;,&#92;&quot;idx&#92;&quot;:5},{&#92;&quot;word&#92;&quot;:&#92;&quot;hi&#92;&quot;,&#92;&quot;idx&#92;&quot;:15},{&#92;&quot;word&#92;&quot;:&#92;&quot;apology&#92;&quot;,&#92;&quot;idx&#92;&quot;:8},{&#92;&quot;word&#92;&quot;:&#92;&quot;possible&#92;&quot;,&#92;&quot;idx&#92;&quot;:26},{&#92;&quot;word&#92;&quot;:&#92;&quot;silly&#92;&quot;,&#92;&quot;idx&#92;&quot;:30},{&#92;&quot;word&#92;&quot;:&#92;&quot;question&#92;&quot;,&#92;&quot;idx&#92;&quot;:27},{&#92;&quot;word&#92;&quot;:&#92;&quot;think&#92;&quot;,&#92;&quot;idx&#92;&quot;:32},{&#92;&quot;word&#92;&quot;:&#92;&quot;adsl&#92;&quot;,&#92;&quot;idx&#92;&quot;:7},{&#92;&quot;word&#92;&quot;:&#92;&quot;service&#92;&quot;,&#92;&quot;idx&#92;&quot;:29}]&quot;' data-total-rows='35' data-total-columns='2' data-max-columns='50' data-banner-text='&quot;&quot;' data-pagination='true' data-page-size='10' data-selection='&quot;multi&quot;' data-show-filters='false' data-show-download='true' data-show-column-summaries='true' data-show-data-types='true' data-show-page-size-selector='true' data-show-column-explorer='true' data-show-chart-builder='true' data-row-headers='[]' data-has-stable-row-id='false' data-lazy='false' data-preload='false'></marimo-table></marimo-ui-element></div><div style='flex: 5'><marimo-ui-element object-id='Hstk-0' random-id='51253c7d-2812-73a2-e4fd-44f0241ac9f1'><marimo-table data-initial-value='[]' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Tokens&lt;/span&gt;&lt;/span&gt;&quot;' data-data='&quot;[{&#92;&quot;Token&#92;&quot;:&#92;&quot;Eircom&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;Eircom&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;PROPN&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;NNP&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;ORG&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;aDSL&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;aDSL&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;PROPN&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;NNP&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;Nat&#x27;ing&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;Nat&#x27;ing&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;PROPN&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;NNP&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;&#92;&#92;n&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;&#92;&#92;n&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;SPACE&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;_SP&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;Hi&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;hi&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;INTJ&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;UH&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;all&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;all&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;PRON&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;DT&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:true,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;,&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;,&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;PUNCT&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;,&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;&#92;&#92;n&#92;&#92;n&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;&#92;&#92;n&#92;&#92;n&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;SPACE&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;_SP&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;apologies&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;apology&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;NOUN&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;NNS&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:false,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false},{&#92;&quot;Token&#92;&quot;:&#92;&quot;for&#92;&quot;,&#92;&quot;Lemma&#92;&quot;:&#92;&quot;for&#92;&quot;,&#92;&quot;PoS&#92;&quot;:&#92;&quot;ADP&#92;&quot;,&#92;&quot;Tag&#92;&quot;:&#92;&quot;IN&#92;&quot;,&#92;&quot;Entity&#92;&quot;:&#92;&quot;&#92;&quot;,&#92;&quot;Stop word&#92;&quot;:true,&#92;&quot;URL&#92;&quot;:false,&#92;&quot;Email&#92;&quot;:false}]&quot;' data-total-rows='176' data-total-columns='8' data-max-columns='50' data-banner-text='&quot;&quot;' data-pagination='true' data-page-size='10' data-selection='&quot;multi&quot;' data-show-filters='false' data-show-download='true' data-show-column-summaries='true' data-show-data-types='true' data-show-page-size-selector='true' data-show-column-explorer='true' data-show-chart-builder='true' data-row-headers='[]' data-wrapped-columns='[&quot;Token&quot;,&quot;Lemma&quot;]' data-has-stable-row-id='false' data-lazy='false' data-preload='false'></marimo-table></marimo-ui-element></div></div><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Stop word ratio: 0.18181818181818182</span></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_tfidf = TfidfVectorizer()\n",
    "_entry = deduplicated_df.iloc[idx.value]\n",
    "\n",
    "_clean_subj = subject_cleanup(_entry['subject'])\n",
    "_clean_html = html_cleanup(_entry['html'])\n",
    "_merged_text = merge_text_html(_entry['text'],_entry['html'])\n",
    "_text = f\"{_clean_subj}\\n{_merged_text}\"\n",
    "\n",
    "_text_2 = artifact_cleanup(_text)\n",
    "\n",
    "_doc = nlp(_text_2)\n",
    "_debug_text = nlp_pipeline(_doc, debug=True)\n",
    "_new_text = nlp_pipeline(_doc)\n",
    "\n",
    "\n",
    "_table2 = mo.ui.table(\n",
    "    data=[{\n",
    "        \"Token\": token.text,\n",
    "        \"Lemma\": token.lemma_,\n",
    "        \"PoS\": token.pos_,\n",
    "        \"Tag\": token.tag_,\n",
    "        \"Entity\": token.ent_type_,\n",
    "        \"Stop word\": token.is_stop,\n",
    "        \"URL\": token.like_url,\n",
    "        \"Email\": token.like_email\n",
    "    } for token in _doc],\n",
    "    label=\"Tokens\",\n",
    "    pagination=True,\n",
    "    wrapped_columns=[\"Token\", \"Lemma\"]\n",
    ")\n",
    "\n",
    "\n",
    "if len(_new_text) == 0:\n",
    "    _new_text = \"_<EMPTY>_\"\n",
    "\n",
    "_results = _tfidf.fit_transform([_new_text])\n",
    "_vocab = [{'word': _word, 'idx': _idx} for _word, _idx in _tfidf.vocabulary_.items()]\n",
    "_table = mo.ui.table(data=_vocab, label=\"TF-IDF\", pagination=True)\n",
    "\n",
    "\n",
    "mo.vstack(\n",
    "    [\n",
    "        idx,\n",
    "        mo.md(f\"Label: {\"Ham\" if _entry['label'] == 0 else \"Spam\"}\"),\n",
    "        mo.hstack(\n",
    "            [\n",
    "                mo.md(f\"{_clean_subj}\\n{_merged_text}\".replace(\"\\n\", \" \")),\n",
    "                mo.md(_debug_text.replace(\"\\n\", \" \")),\n",
    "                mo.md(_new_text.replace(\"\\n\", \" \"))\n",
    "            ],\n",
    "            widths=[1, 1.2, 0.9]\n",
    "        ),\n",
    "        mo.hstack([\n",
    "            _table,\n",
    "            _table2\n",
    "        ], widths=[1, 5]),\n",
    "        mo.md(f\"Stop word ratio: {len([tok for tok in _doc if tok.is_stop]) / _doc.__len__()}\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"d-pre-cleanup\">d. Pre cleanup</h3>\n",
       "<span class=\"paragraph\">Find and replace certain tokens with regex.</span>\n",
       "<span class=\"paragraph\">For now, in some emails' subject line, there is mailing list information. E.g.: [ILUG], or [zzzteana].\n",
       "<br>\n",
       "This should be removed, otherwise the model would learn them as ham indicator.</span>\n",
       "<span class=\"paragraph\">It's not technically wrong, but it doesn't generalize. Those mailing lists are specific to this dataset.</span>\n",
       "<span class=\"paragraph\">Another artifact is <code>--DeathToSpamDeathToSpamDeathToSpam--</code>.\n",
       "<br>\n",
       "It is suspected to be related to the spam report campaign, where people forward spam emails to the collector, who built this dataset.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    pre_cleaned_df = load_dataset('pre_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    tqdm.pandas(desc=\"Running pre_cleanup\", ncols=100)\n",
    "\n",
    "    pre_cleaned_df = pd.DataFrame({\n",
    "        'text': deduplicated_df.progress_apply(\n",
    "            lambda x: pre_cleanup(x['subject'], x['text'], x['html']),\n",
    "            axis=1\n",
    "        ),\n",
    "        'label': deduplicated_df['label']\n",
    "    })\n",
    "\n",
    "    save_dataset(pre_cleaned_df, 'pre_cleaned')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### d. Pre cleanup\n",
    "\n",
    "Find and replace certain tokens with regex.\n",
    "\n",
    "For now, in some emails' subject line, there is mailing list information. E.g.: [ILUG], or [zzzteana].\n",
    "<br>\n",
    "This should be removed, otherwise the model would learn them as ham indicator.\n",
    "\n",
    "It's not technically wrong, but it doesn't generalize. Those mailing lists are specific to this dataset.\n",
    "\n",
    "Another artifact is `--DeathToSpamDeathToSpamDeathToSpam--`.\n",
    "<br>\n",
    "It is suspected to be related to the spam report campaign, where people forward spam emails to the collector, who built this dataset.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">This is the <code>pre_cleanup</code> function. It will remove the mailing list information, merge plain text and HTML into a single container, and remove artifact.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pre_cleanup(subject: str, text: str, html: str):\n",
    "    clean_subj = subject_cleanup(subject)\n",
    "    merged_text = merge_text_html(text, html)\n",
    "\n",
    "    pass_1 = f\"{clean_subj}\\n{merged_text}\"\n",
    "    pass_2 = artifact_cleanup(pass_1)\n",
    "\n",
    "    return pass_2\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "This is the `pre_cleanup` function. It will remove the mailing list information, merge plain text and HTML into a single container, and remove artifact.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Some emails have either plain text or HTML content, or both.\n",
       "<br>\n",
       "Some emails have the same content in both plain text and HTML.</span>\n",
       "<span class=\"paragraph\">If the plain text and HTML content have a <a href=\"https://www.geeksforgeeks.org/python/python-similarity-metrics-of-strings/\" rel=\"noopener noreferrer\" target=\"_blank\">cosine similarity</a> of greater than <code>threshold</code>, they will be considered to be the same.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_text_html(text: str, html: str, threshold = 0.95) -> str:\n",
    "    if len(html) != 0:\n",
    "        html = html_cleanup(html)\n",
    "\n",
    "        # Some texts has leftover HTML somehow\n",
    "        text = html_cleanup(text)\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return html\n",
    "\n",
    "        else:\n",
    "            similarity = cosine_similarity(text, html)\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                return text\n",
    "\n",
    "            else:\n",
    "                return f\"{text}\\n{html}\"\n",
    "\n",
    "    else:\n",
    "        return html_cleanup(text)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "Some emails have either plain text or HTML content, or both.\n",
    "<br>\n",
    "Some emails have the same content in both plain text and HTML.\n",
    "\n",
    "If the plain text and HTML content have a [cosine similarity](https://www.geeksforgeeks.org/python/python-similarity-metrics-of-strings/) of greater than `threshold`, they will be considered to be the same.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">While parsing HTML content, links (<code>&lt;a&gt;</code> tags) will be lost.\n",
       "<br>\n",
       "The function adds \"google.com\" for each link found, which will be interpreted as an <code>_url_</code> token.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# url_pattern = re.compile(\n",
    "#     r\"((http|ftp|https):\\/\\/)?([\\w_-]+(?:\\.[\\w_-]+)*\\.[a-zA-Z_-][\\w_-]+)([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    "# )\n",
    "\n",
    "# Remove mailing list information\n",
    "subject_cleanup_pattern = re2.compile(\n",
    "    r\"\\[.*?\\]\"\n",
    ")\n",
    "\n",
    "\n",
    "def artifact_cleanup(text):\n",
    "    artifact = \"--DeathToSpamDeathToSpamDeathToSpam--\"\n",
    "    return text.replace(artifact, \"\")\n",
    "\n",
    "\n",
    "def subject_cleanup(text: str) -> str:\n",
    "    return re2.sub(\n",
    "        pattern=subject_cleanup_pattern,\n",
    "        text=text,\n",
    "        repl=\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def html_cleanup(html):\n",
    "    if pd.isna(html):\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    # Artificially add _URL_ tokens\n",
    "    return soup.get_text(separator=\" \", strip=True) + \" google.com\" * len(soup.find_all('a'))\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "While parsing HTML content, links (`<a>` tags) will be lost.\n",
    "<br>\n",
    "The function adds \"google.com\" for each link found, which will be interpreted as an `_url_` token.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">From the GeeksForGeeks page mentioned above.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cosine_similarity(s1: str, s2: str) -> float:\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert strings to character frequency vectors\n",
    "    vec1 = Counter(s1)\n",
    "    vec2 = Counter(s2)\n",
    "\n",
    "    # Calculating cosine similarity\n",
    "    dot_product = sum(vec1[ch] * vec2[ch] for ch in vec1)\n",
    "    magnitude1 = sqrt(sum(count ** 2 for count in vec1.values()))\n",
    "    magnitude2 = sqrt(sum(count ** 2 for count in vec2.values()))\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "From the GeeksForGeeks page mentioned above.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"e-post-cleanup\">e. Post cleanup</h3>\n",
       "<span class=\"paragraph\">Find and replace tokens with Natural Language Processing (NLP).</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_CHAR_LENGTH = 15000\n",
    "\n",
    "\n",
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    post_cleaned_df = load_dataset('post_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    _post_cleaned_emails = []\n",
    "    _truncated_texts = []\n",
    "    _skipped_idx = []\n",
    "\n",
    "    for i, row in pre_cleaned_df.iterrows():\n",
    "        if len(row['text']) > MAX_CHAR_LENGTH:\n",
    "            _skipped_idx.append(i)\n",
    "            _truncated_texts.append(row['text'][:MAX_CHAR_LENGTH])\n",
    "        else:\n",
    "            _truncated_texts.append(row['text'])\n",
    "\n",
    "    _post_cleanup_status = mo.status.progress_bar(\n",
    "        title=\"Running post_cleanup\",\n",
    "        total=len(pre_cleaned_df)\n",
    "    )\n",
    "\n",
    "    with _post_cleanup_status as _bar:\n",
    "        for _doc in process_batches(\n",
    "            nlp=nlp,\n",
    "            texts=_truncated_texts,\n",
    "            batch_size=5\n",
    "        ):\n",
    "            _post_cleaned_emails.append(nlp_pipeline(_doc))\n",
    "\n",
    "            _bar.update()\n",
    "\n",
    "    post_cleaned_df = pd.DataFrame({\n",
    "        'text': _post_cleaned_emails,\n",
    "        'label': pre_cleaned_df['label']\n",
    "    })\n",
    "\n",
    "    pre_cleaned_df[post_cleaned_df['text'] == \"\"]\n",
    "    post_cleaned_df = post_cleaned_df[post_cleaned_df['text'] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    save_dataset(post_cleaned_df, 'post_cleaned')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### e. Post cleanup\n",
    "\n",
    "Find and replace tokens with Natural Language Processing (NLP).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def process_batches(nlp, texts, batch_size=None):\n",
    "    for i, doc in enumerate(nlp.pipe(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        disable=[\"parser\"]\n",
    "    )):\n",
    "        doc._.trf_data = None\n",
    "\n",
    "        yield doc\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            torch.cuda.empty_cache() # VRAM explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email matching\n",
    "email_pattern = re2.compile(\n",
    "    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    ")\n",
    "\n",
    "pgp_signature_pattern = re2.compile(\n",
    "    r\"(?s)-----BEGIN PGP SIGNATURE-----.*?-----END PGP SIGNATURE-----\"\n",
    ")\n",
    "\n",
    "\n",
    "def nlp_pipeline(doc: spacy.tokens.Doc, stop_threshold = 0.01, debug = False) -> str:\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in email_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"EMAIL\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in pgp_signature_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"PGP_SIG\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"DATE\", \"TIME\", \"MONEY\"]:\n",
    "                retokenizer.merge(ent)\n",
    "\n",
    "\n",
    "    # Treat as spam\n",
    "    stop_word_ratio = len([token for token in doc if token.is_stop]) / doc.__len__()\n",
    "    if stop_word_ratio < stop_threshold:\n",
    "        return \"\"\n",
    "\n",
    "    return \" \".join(\n",
    "        [token_processor(token, debug) for token in doc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "name": "*token_processor"
    }
   },
   "outputs": [],
   "source": [
    "def token_processor(token: spacy.tokens.Token, debug: bool = False) -> str:\n",
    "    def debug_fmt(tag: str) -> str:\n",
    "        spaced_text = \"~ ~\".join(token.text.split())\n",
    "        return f\"{tag} ~<-{spaced_text}~\"\n",
    "\n",
    "    if token.is_stop or token.tag_ == \"LS\":\n",
    "        return \"\" if not debug else f\"~~{token.text}~~\"\n",
    "\n",
    "    if token.like_url:\n",
    "        return \"_url_\" if not debug else debug_fmt(\"_url_\")\n",
    "\n",
    "    if token.like_email:\n",
    "        return \"_email_\" if not debug else debug_fmt(\"_email_\")\n",
    "\n",
    "    if token.ent_type_ and token.ent_type_ not in [\"CARDINAL\", \"ORDINAL\"]:\n",
    "        tag = f\"_{token.ent_type_}_\".lower()\n",
    "        return tag if not debug else debug_fmt(tag)\n",
    "\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        return \"_propn_\" if not debug else debug_fmt(\"_propn_\")\n",
    "\n",
    "    if token.pos_ == \"NUM\":\n",
    "        return \"_num_\" if not debug else debug_fmt(\"_num_\")\n",
    "\n",
    "    if token.pos_ == \"X\" and token.tag_ == \"FW\":\n",
    "        return \"_foreign_\" if not debug else debug_fmt(\"_foreign_\")\n",
    "\n",
    "    return token.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# II. Features engineering\n",
    "\n",
    "## 1. Additional features to consider\n",
    "\n",
    "- ~~Contains HTML (boolean)~~\n",
    "- Number of links (share the same token with links in text)\n",
    "- ~~Number of special characters~~ Might not be a good feature\n",
    "- Capitals ratio\n",
    "- ~~Email size~~ (maybe not)\n",
    "- Reply ratio, reply depth count\n",
    "- Stop word ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"a-capitals-ratio\">a. Capitals ratio</h3>\n",
       "<span class=\"paragraph\">I didn't use this yet, though.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_capitals_ratio(text: str) -> float:\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    capitals_count = sum(1 for c in text if c.isupper())\n",
    "    return capitals_count / len(text)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### a. Capitals ratio\n",
    "\n",
    "I didn't use this yet, though.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-number-of-special-characters\">b. Number of special characters</h3>\n",
       "<span class=\"paragraph\">On second thought..., I skipped it.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_special_chars(text):\n",
    "    special_chars = \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>?/`~\"\n",
    "    return sum(1 for char in text if char in special_chars)\n",
    "\n",
    "#df['text'].apply(count_special_chars)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Number of special characters\n",
    "\n",
    "On second thought..., I skipped it.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"c-tf-idf\">c. TF-IDF</h3>\n",
       "<span class=\"paragraph\">Code copied from GeeksForGeeks:</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the data and transform it into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(post_cleaned_df['text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### c. TF-IDF\n",
    "\n",
    "Code copied from GeeksForGeeks:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df\n",
    "y = post_cleaned_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "# III. Model training\n",
    "\n",
    "## 1. Train time\n",
    "\n",
    "Here, we train our Logistic Regression model with 75-25 train-test split.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Confusion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.97      0.99      0.98      1033\n",
      "        Spam       0.97      0.92      0.94       417\n",
      "\n",
      "    accuracy                           0.97      1450\n",
      "   macro avg       0.97      0.95      0.96      1450\n",
      "weighted avg       0.97      0.97      0.97      1450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = [\"Ham\", \"Spam\"]\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {},
   "source": [
    "# IV. Model analysis\n",
    "\n",
    "We take a look into our model and see its decision-making.\n",
    "\n",
    "## 1. Important words\n",
    "\n",
    "The charts show top 20 words for words that are likely in ham and spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame({\n",
    "    'word': tfidf_df.columns,\n",
    "    'weight': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "\n",
    "top_spam = features_df.nlargest(20, 'weight')\n",
    "top_ham = features_df.nsmallest(20, 'weight')\n",
    "\n",
    "\n",
    "plot_df = pd.concat([\n",
    "    top_spam.assign(type='Spam'),\n",
    "    top_ham.assign(type='Ham')\n",
    "])\n",
    "\n",
    "plot_df['type'] = plot_df['weight'].apply(lambda x: 'Spam' if x > 0 else 'Ham')\n",
    "\n",
    "\n",
    "mo.hstack(\n",
    "    [\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_ham).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='greens', reverse=True, type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Ham words\",\n",
    "                height=600\n",
    "            )\n",
    "        ),\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_spam).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='reds', type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Spam words\",\n",
    "                height=600\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    justify='center',\n",
    "    widths=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TRpd",
   "metadata": {},
   "source": [
    "# Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='TXez-0' random-id='05a87ac0-735e-c603-7ead-738b46e3b183'><marimo-dataframe data-initial-value='{&quot;transforms&quot;:[]}' data-label='null' data-columns='[[&quot;text&quot;,&quot;string&quot;,&quot;object&quot;],[&quot;label&quot;,&quot;integer&quot;,&quot;int64&quot;]]' data-dataframe-name='&quot;pre_cleaned_df&quot;' data-total='5851' data-page-size='5' data-show-download='true'></marimo-dataframe></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.ui.dataframe(pre_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='dNNg-0' random-id='e40643dc-f05a-0d25-f840-207a223769b6'><marimo-table data-initial-value='[]' data-label='null' data-data='&quot;[{&#92;&quot;&#92;&quot;:0,&#92;&quot;text&#92;&quot;:1576},{&#92;&quot;&#92;&quot;:1,&#92;&quot;text&#92;&quot;:908},{&#92;&quot;&#92;&quot;:2,&#92;&quot;text&#92;&quot;:1760},{&#92;&quot;&#92;&quot;:3,&#92;&quot;text&#92;&quot;:1157},{&#92;&quot;&#92;&quot;:4,&#92;&quot;text&#92;&quot;:1083},{&#92;&quot;&#92;&quot;:5,&#92;&quot;text&#92;&quot;:773},{&#92;&quot;&#92;&quot;:6,&#92;&quot;text&#92;&quot;:1355},{&#92;&quot;&#92;&quot;:7,&#92;&quot;text&#92;&quot;:1197},{&#92;&quot;&#92;&quot;:8,&#92;&quot;text&#92;&quot;:6116},{&#92;&quot;&#92;&quot;:9,&#92;&quot;text&#92;&quot;:893}]&quot;' data-total-rows='5851' data-total-columns='1' data-max-columns='50' data-banner-text='&quot;&quot;' data-pagination='true' data-page-size='10' data-field-types='[[&quot;text&quot;,[&quot;integer&quot;,&quot;int64&quot;]]]' data-show-filters='true' data-show-download='true' data-show-column-summaries='true' data-show-data-types='true' data-show-page-size-selector='true' data-show-column-explorer='true' data-show-chart-builder='true' data-row-headers='[[&quot;&quot;,[&quot;integer&quot;,&quot;int64&quot;]]]' data-has-stable-row-id='false' data-lazy='false' data-preload='false'></marimo-table></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_cleaned_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {},
   "source": [
    "## Beautiful Soup playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='wlCL-0' random-id='2f6c3be9-24e7-546e-41d4-8380d58ddf01'><marimo-text-area data-initial-value='&quot;&quot;' data-label='null' data-placeholder='&quot;Paste HTML code...&quot;' data-disabled='false' data-debounce='true' data-full-width='true'></marimo-text-area></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_input = mo.ui.text_area(placeholder=\"Paste HTML code...\", full_width=True)\n",
    "html_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_input.value, 'html5lib')\n",
    "cleaned_html = soup.get_text(separator=\" \", strip=True)\n",
    "print(cleaned_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {},
   "source": [
    "## RE2 playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='rEll-0' random-id='19f35491-4ec6-796a-a86c-910953121ce4'><marimo-text-area data-initial-value='&quot;&quot;' data-label='null' data-placeholder='&quot;Paste text...&quot;' data-disabled='false' data-debounce='true' data-full-width='true'></marimo-text-area></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_input = mo.ui.text_area(placeholder=\"Paste text...\", full_width=True)\n",
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='dGlV-0' random-id='d1ae01e4-ecfb-c991-7b70-36a5b282a58c'><marimo-text data-initial-value='&quot;&quot;' data-label='null' data-placeholder='&quot;Regex&quot;' data-kind='&quot;text&quot;' data-full-width='true' data-disabled='false' data-debounce='true'></marimo-text></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regex_input = mo.ui.text(placeholder=\"Regex\", full_width=True)\n",
    "regex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class='text-xs'>&#x27;&#x27;</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re2.sub(pattern=regex_input.value, text=text_input.value, repl=\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
