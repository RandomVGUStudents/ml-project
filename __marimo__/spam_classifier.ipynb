{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "marimo": {
     "name": "setup"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/mgm19ngyq575kh4dhwm18kq138h58lnh-python3-3.13.9-env/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import marimo as mo\n",
    "\n",
    "# Miscellaneous\n",
    "import glob\n",
    "import hashlib\n",
    "import os\n",
    "import tarfile\n",
    "import typing\n",
    "import wget\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import re\n",
    "import re2\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import mailparser\n",
    "\n",
    "# Plotting\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Load NLP module\n",
    "print(spacy.prefer_gpu())\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\"label\": \"TIME\", \"pattern\": [{\"SHAPE\": \"dd:dd:dd\"}]},\n",
    "]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Hi there!\n",
    "\n",
    "This is the notebook for our Machine Learning project.\n",
    "<br>\n",
    "Let's take a look at the table of content (it is also available on the right side of the screen):\n",
    "\n",
    "I. Data Acquisition\n",
    "- Download dataset\n",
    "- Load whole dataset\n",
    "- Preprocess email\n",
    "\n",
    "II. Feature Engineering\n",
    "- Additional features to consider\n",
    "\n",
    "III. Training\n",
    "- Train time\n",
    "- Confusion time\n",
    "\n",
    "IV. Model Analysis\n",
    "\n",
    "**Let's get started!**\n",
    "\n",
    "üí° Tips: Click ‚ñ∂Ô∏è in the down right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# I. Data Acquisition\n",
    "\n",
    "In this section, we obtain the data from the Apache's SpamAssassin Public Corpus and read them.\n",
    "<br>\n",
    "After that, we can load all emails into a DataFrame.\n",
    "\n",
    "## 1. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"a-define-dataset-source-and-download-function\">a. Define dataset source and download function</h3>\n",
       "<span class=\"paragraph\">We have 5 files:</span>\n",
       "<ul>\n",
       "<li>easy_ham (2500 ham emails)</li>\n",
       "<li>easy_ham_2 (1400 ham emails)</li>\n",
       "<li>hard_ham (250 ham emails)</li>\n",
       "<li>spam (500 spam emails)</li>\n",
       "<li>spam_2 (1396 spam emails) <em>Although they said there were 1397 emails, there are only 1396 files in the extracted folder.</em></li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">Our download function will download the files and extract into <code>./datasets/</code> folder.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_source = {\n",
    "    'easy_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\",\n",
    "        'count': 2500,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'easy_ham_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\",\n",
    "        'count': 1400,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'hard_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\",\n",
    "        'count': 250,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'spam': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\",\n",
    "        'count': 500,\n",
    "        'is_spam': True\n",
    "    },\n",
    "    'spam_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\",\n",
    "        'count': 1396,\n",
    "        'is_spam': True\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_dir = \"./datasets/\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "\n",
    "def download_dataset(dataset_path: str, dataset_url: str):\n",
    "    tmp = wget.download(dataset_url)\n",
    "\n",
    "    with tarfile.open(tmp, \"r:bz2\") as tar:\n",
    "        tar.extractall(dataset_dir)\n",
    "\n",
    "    os.remove(os.path.join(dataset_path, \"cmds\"))\n",
    "    os.remove(tmp)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### a. Define dataset source and download function\n",
    "\n",
    "We have 5 files:\n",
    "\n",
    "- easy_ham (2500 ham emails)\n",
    "- easy_ham_2 (1400 ham emails)\n",
    "- hard_ham (250 ham emails)\n",
    "- spam (500 spam emails)\n",
    "- spam_2 (1396 spam emails) *Although they said there were 1397 emails, there are only 1396 files in the extracted folder.*\n",
    "\n",
    "Our download function will download the files and extract into `./datasets/` folder.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-check-dataset-integrity\">b. Check dataset integrity</h3>\n",
       "<span class=\"paragraph\">The file name of each email is its MD5 hash.\n",
       "<br>\n",
       "We can use it to verify the integrity of the dataset.</span>\n",
       "<span class=\"paragraph\"><em>Note: A file has incorrect hash from source, that's not our fault.</em></span>\n",
       "<span class=\"paragraph\">We will redownload the dataset if any file is corrupted or missing.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _dataset_name, _dataset_info in dataset_source.items():\n",
    "    _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "    if os.path.exists(_dataset_path):\n",
    "\n",
    "        if len(os.listdir(_dataset_path)) != _dataset_info['count']:\n",
    "            print(f\"Dataset {_dataset_name} is missing some files, let's redownload.\")\n",
    "            download_dataset(_dataset_name, _dataset_info['url'])\n",
    "\n",
    "        for _filename in os.listdir(_dataset_path):\n",
    "            _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "            if os.path.isfile(_file_path):\n",
    "                _md5_hash = hashlib.md5(open(_file_path, 'rb').read()).hexdigest()\n",
    "                _provided_hash = _filename.split(\".\")[1]\n",
    "\n",
    "                # Ignore wrong hash from source\n",
    "                if (_md5_hash != _provided_hash and _provided_hash != \"244a63cd74c81123ef26129453e32c95\"):\n",
    "                    print(f\"Hash mismatch for {_filename}, redownloading dataset {_dataset_name}...\")\n",
    "\n",
    "                    download_dataset(_dataset_name, _dataset_info['url'])\n",
    "                    os.remove(os.path.join(_dataset_path, _filename))\n",
    "\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        print(f\"Missing dataset {_dataset_name}, downloading it now...\")\n",
    "        download_dataset(_dataset_path, _dataset_info['url'])\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Check dataset integrity\n",
    "\n",
    "The file name of each email is its MD5 hash.\n",
    "<br>\n",
    "We can use it to verify the integrity of the dataset.\n",
    "\n",
    "*Note: A file has incorrect hash from source, that's not our fault.*\n",
    "\n",
    "We will redownload the dataset if any file is corrupted or missing.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"c-viewing-some-emails\">c. Viewing some emails</h3>\n",
       "<span class=\"paragraph\">Let's take a look at some emails that we've got.</span>\n",
       "<span class=\"paragraph\">To save us some hassle, we won't be looking as raw email files (feel free to look at them, though!)\n",
       "<br>\n",
       "However, we will be using the <code>mailparser</code> library to parse the emails.</span>\n",
       "<span class=\"paragraph\">What we care about:</span>\n",
       "<ul>\n",
       "<li>Subject</li>\n",
       "<li>Plain text content</li>\n",
       "<li>HTML content</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">üí° Tips: Click \"Fullscreen\" or \"Expand output\" button (they will appear on the right side when you hover the box below).\n",
       "<br>\n",
       "Modify the number to view different emails.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mails = glob.glob(\"./datasets/*/*\", recursive=True)\n",
    "idx = mo.ui.number(start=0, stop=len(mails) - 1, label=\"Number\")\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### c. Viewing some emails\n",
    "\n",
    "Let's take a look at some emails that we've got.\n",
    "\n",
    "To save us some hassle, we won't be looking as raw email files (feel free to look at them, though!)\n",
    "<br>\n",
    "However, we will be using the `mailparser` library to parse the emails.\n",
    "\n",
    "What we care about:\n",
    "\n",
    "- Subject\n",
    "- Plain text content\n",
    "- HTML content\n",
    "\n",
    "üí° Tips: Click \"Fullscreen\" or \"Expand output\" button (they will appear on the right side when you hover the box below).\n",
    "<br>\n",
    "Modify the number to view different emails.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex;flex: 1;flex-direction: column;justify-content: flex-start;align-items: normal;flex-wrap: nowrap;gap: 0.5rem'><marimo-ui-element object-id='PKri-0' random-id='05efe69e-f9fb-7dbe-fd02-069ed63e5e2e'><marimo-number data-initial-value='0' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Number&lt;/span&gt;&lt;/span&gt;&quot;' data-start='0' data-stop='6045' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element><span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Path: <code>./datasets/easy_ham/00008.5891548d921601906337dcf1ed8543cb</code></span>\n",
       "<div class=\"language-text codehilite\"><pre><span></span><code>-- SUBJ --\n",
       "Re: [zzzzteana] Nothing like mama used to make**\n",
       "-- TEXT --\n",
       "Martin Adamson wrote:\n",
       "&gt; \n",
       "&gt; Isn&#39;t it just basically a mixture of beaten egg and bacon (or pancetta, \n",
       "&gt; really)? You mix in the raw egg to the cooked pasta and the heat of the pasta \n",
       "&gt; cooks the egg. That&#39;s my understanding.\n",
       "&gt; \n",
       "\n",
       "You&#39;re probably right, mine&#39;s just the same but with the cream added to the \n",
       "eggs.  I guess I should try it without.  Actually looking on the internet for a \n",
       "recipe I found this one from possibly one of the scariest people I&#39;ve ever seen, \n",
       "and he&#39;s a US Congressman:\n",
       "&lt;http://www.virtualcities.com/ons/me/gov/megvjb1.htm&gt;\n",
       "\n",
       "That&#39;s one of the worst non-smiles ever.\n",
       "\n",
       "Stew\n",
       "ps. Apologies if any of the list&#39;s Maine residents voted for this man, you won&#39;t \n",
       "do it again once you&#39;ve seen this pic.\n",
       "\n",
       "-- \n",
       "Stewart Smith\n",
       "Scottish Microelectronics Centre, University of Edinburgh.\n",
       "http://www.ee.ed.ac.uk/~sxs/\n",
       "\n",
       "\n",
       "------------------------ Yahoo! Groups Sponsor ---------------------~--&gt;\n",
       "4 DVDs Free +s&amp;p Join Now\n",
       "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
       "---------------------------------------------------------------------~-&gt;\n",
       "\n",
       "To unsubscribe from this group, send an email to:\n",
       "forteana-unsubscribe@egroups.com\n",
       "\n",
       "\n",
       "\n",
       "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "-- HTML --\n",
       "&lt;No HTML content&gt;\n",
       "</code></pre></div></span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mail = mailparser.parse_from_file(mails[idx.value])\n",
    "\n",
    "\n",
    "mo.vstack([\n",
    "    idx,\n",
    "    mo.md(f\"\"\"\n",
    "Path: `{mails[idx.value]}`\n",
    "\n",
    "```text\n",
    "-- SUBJ --\n",
    "{mail.subject}**\n",
    "-- TEXT --\n",
    "{\"---\\n\\n---\".join(mail.text_plain) if mail.text_plain else \"<No plain text content>\"}\n",
    "-- HTML --\n",
    "{mail.text_html if mail.text_html else \"<No HTML content>\"}\n",
    "```\n",
    "\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"2-load-whole-dataset\">2. Load whole dataset</h2>\n",
       "<span class=\"paragraph\">Now let's load everything into a DataFrame.</span>\n",
       "<span class=\"paragraph\">Let's understand why we need checkpoints.\n",
       "<br>\n",
       "We don't want to reprocess the dataset every time we run the notebook, so we will save the processed dataset into a file.\n",
       "<br>\n",
       "In subsequent runs, we can just load it back.</span>\n",
       "<span class=\"paragraph\">Here we have some checkpoints and some functions to save and load datasets.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_checkpoints = {\n",
    "    'orig': {\n",
    "        'description': \"Dataset without preprocessing (6046 entries)\",\n",
    "        'checksum': \"6ab4e23a696aeac72ad3b38396666a25\"\n",
    "    },\n",
    "    'pre_cleaned': {\n",
    "        'description': \"Dataset after pre-cleanup (5851 entries)\",\n",
    "        'checksum': \"589f556bc02bdeaa376ca92a99e9755c\"\n",
    "    },\n",
    "    'post_cleaned': {\n",
    "        'description': \"Dataset after post-cleanup (5851 entries)\",\n",
    "        'checksum': \"d972cc17841c34ed3f73c1bd3e3fad62\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(checkpoint: str) -> pd.DataFrame:\n",
    "    dataset_path = os.path.join(dataset_dir, f\"{checkpoint}.gzip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"No checkpoint '{checkpoint}' in {dataset_dir}\")\n",
    "\n",
    "    hash = hashlib.md5(open(dataset_path, 'rb').read()).hexdigest()\n",
    "    if hash != dataset_checkpoints[checkpoint]['checksum']:\n",
    "        print(f\"Expected: {dataset_checkpoints[checkpoint]['checksum']}, got: {hash}\")\n",
    "        raise ValueError(\"Dataset file broken.\")\n",
    "\n",
    "    data = pd.read_parquet(dataset_path)\n",
    "    return data\n",
    "\n",
    "def save_dataset(df: pd.DataFrame, checkpoint_name: str):\n",
    "    path = os.path.join(dataset_dir, f\"{checkpoint_name}.gzip\")\n",
    "    df.to_parquet(\n",
    "        path=path,\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    hash = hashlib.md5(open(path, 'rb').read()).hexdigest()\n",
    "    print(f\"File: {path}\\nMD5: {hash}\")\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "## 2. Load whole dataset\n",
    "\n",
    "Now let's load everything into a DataFrame.\n",
    "\n",
    "Let's understand why we need checkpoints.\n",
    "<br>\n",
    "We don't want to reprocess the dataset every time we run the notebook, so we will save the processed dataset into a file.\n",
    "<br>\n",
    "In subsequent runs, we can just load it back.\n",
    "\n",
    "Here we have some checkpoints and some functions to save and load datasets.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">We use <code>mailparser</code> to parse 6k+ emails into a DataFrame, and save it for later.</span>\n",
       "<span class=\"paragraph\">We will only do that when the checkpoint isn't available, or is somehow broken.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df = load_dataset('orig')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    data = {\n",
    "        'subject': [],\n",
    "        'text': [],\n",
    "        'html': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for _dataset_name, _dataset_info in dataset_source.items():\n",
    "        _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "        if os.path.exists(_dataset_path):\n",
    "\n",
    "            for _filename in os.listdir(_dataset_path):\n",
    "                _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "                if os.path.isfile(_file_path):\n",
    "                    try:\n",
    "                        _mail = mailparser.parse_from_file(_file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with file {_file_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    data['subject'].append(_mail.subject)\n",
    "                    data['text'].append(\"\\n\".join(_mail.text_plain))\n",
    "                    data['html'].append(\"\\n\".join(_mail.text_html))\n",
    "                    data['label'].append(int(_dataset_info['is_spam']))\n",
    "\n",
    "    print(f\"{str(e)}\\nLoading from files...\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    save_dataset(df, 'orig')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "We use `mailparser` to parse 6k+ emails into a DataFrame, and save it for later.\n",
    "\n",
    "We will only do that when the checkpoint isn't available, or is somehow broken.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Preprocess email\n",
    "\n",
    "A little rant about how ML works in this problem:\n",
    "\n",
    "- Emails are broken into 'tokens'. Tokens are words (in simple form), or special ones.\n",
    "- The tokens are the features for the model.\n",
    "\n",
    "Here's what we need to do:\n",
    "\n",
    "1. Remove duplicates from raw dataset.\n",
    "2. Find and replace certain tokens with regex (email, addresses, phone number, etc.)\n",
    "3. Find and replace proper noun tokens (entities)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### a. Data exploration notes\n",
    "\n",
    "- High correlation: HTML usage and spam\n",
    "- Dataset artifact: a lot of spam emails have this:\n",
    "\n",
    "```text\n",
    "--DeathToSpamDeathToSpamDeathToSpam--\n",
    "```\n",
    "\n",
    "- There might be (mostly in spam emails) forms with `_____` blanks.\n",
    "- Mailing list footer should be removed, they mostly appear in ham emails.\n",
    "\n",
    "```text\n",
    "-------------------------------------------------------\n",
    "This sf.net email is sponsored by:ThinkGeek\n",
    "Welcome to geek heaven.\n",
    "http://thinkgeek.com/sf\n",
    "_______________________________________________\n",
    "Spamassassin-talk mailing list\n",
    "Spamassassin-talk@lists.sourceforge.net\n",
    "https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
    "```\n",
    "\n",
    "- ~~Email replies appear with `> ` or `--] `, and \"On DATE, USER wrote:\"~~ Nvm, it's pretty random.\n",
    "- Mailling list (or some name) in subject that is not filtered (like ilug)\n",
    "- Entity labels exclusion: \"CARDINAL\", \"ORDINAL\"\n",
    "- Some emails are malformed, they are extremely long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-remove-duplicates\">b. Remove duplicates</h3></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deduplicated_df = df.drop_duplicates(subset=['text', 'html'])\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Remove duplicates\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### c. Preprocess preview\n",
    "\n",
    "This is merely the scratchpad for whatever we want to do with these emails.\n",
    "\n",
    "Change the number to view different emails, just like above.\n",
    "<br>\n",
    "*(Also don't forget to use fullscreen or expand output)*\n",
    "\n",
    "The first column contains subject + plain text + parsed HTML (more information down below).\n",
    "<br>\n",
    "The second column shows how each token is transformed (in `nlp_pipeline` section down below).\n",
    "<br>\n",
    "The third one shows the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<span class=\"codehilite\"><div class=\"highlight\"><pre><span></span><span class=\"gt\">Traceback (most recent call last):</span>\n",
      "  File <span class=\"nb\">&quot;/tmp/marimo_23769/__marimo__cell_ulZA_.py&quot;</span>, line <span class=\"m\">12</span>, in <span class=\"n\">&lt;module&gt;</span>\n",
      "<span class=\"w\">    </span><span class=\"n\">_debug_text</span> <span class=\"o\">=</span> <span class=\"n\">nlp_pipeline</span><span class=\"p\">(</span><span class=\"n\">_doc</span><span class=\"p\">,</span> <span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
      "  File <span class=\"nb\">&quot;/tmp/marimo_23769/__marimo__cell_TXez_.py&quot;</span>, line <span class=\"m\">3</span>, in <span class=\"n\">nlp_pipeline</span>\n",
      "<span class=\"w\">    </span><span class=\"k\">for</span> <span class=\"n\">match</span> <span class=\"ow\">in</span> <span class=\"n\">email_pattern</span><span class=\"o\">.</span><span class=\"n\">finditer</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">):</span>\n",
      "<span class=\"w\">                 </span><span class=\"pm\">^^^^^^^^^^^^^</span>\n",
      "<span class=\"gr\">NameError</span>: <span class=\"n\">name &#39;email_pattern&#39; is not defined. Did you mean: &#39;url_pattern&#39;?</span>\n",
      "</pre></div>\n",
      "</span>"
     ]
    }
   ],
   "source": [
    "_tfidf = TfidfVectorizer()\n",
    "_entry = deduplicated_df.iloc[idx.value]\n",
    "\n",
    "_clean_subj = subject_cleanup(_entry['subject'])\n",
    "_clean_html = html_cleanup(_entry['html'])\n",
    "_merged_text = merge_text_html(_entry['text'],_entry['html'])\n",
    "_text = f\"{_clean_subj}\\n{_merged_text}\"\n",
    "\n",
    "_text_2 = artifact_cleanup(_text)\n",
    "\n",
    "_doc = nlp(_text_2)\n",
    "_debug_text = nlp_pipeline(_doc, debug=True)\n",
    "_new_text = nlp_pipeline(_doc)\n",
    "\n",
    "\n",
    "_table2 = mo.ui.table(\n",
    "    data=[{\n",
    "        \"Token\": token.text,\n",
    "        \"Lemma\": token.lemma_,\n",
    "        \"PoS\": token.pos_,\n",
    "        \"Tag\": token.tag_,\n",
    "        \"Entity\": token.ent_type_,\n",
    "        \"Stop word\": token.is_stop,\n",
    "        \"URL\": token.like_url,\n",
    "        \"Email\": token.like_email\n",
    "    } for token in _doc],\n",
    "    label=\"Tokens\",\n",
    "    pagination=True,\n",
    "    wrapped_columns=[\"Token\", \"Lemma\"]\n",
    ")\n",
    "\n",
    "\n",
    "if len(_new_text) == 0:\n",
    "    _new_text = \"_<EMPTY>_\"\n",
    "\n",
    "_results = _tfidf.fit_transform([_new_text])\n",
    "_vocab = [{'word': _word, 'idx': _idx} for _word, _idx in _tfidf.vocabulary_.items()]\n",
    "_table = mo.ui.table(data=_vocab, label=\"TF-IDF\", pagination=True)\n",
    "\n",
    "\n",
    "mo.vstack(\n",
    "    [\n",
    "        idx,\n",
    "        mo.md(f\"Label: {\"Ham\" if _entry['label'] == 0 else \"Spam\"}\"),\n",
    "        mo.hstack(\n",
    "            [\n",
    "                mo.md(f\"{_clean_subj}\\n{_merged_text}\".replace(\"\\n\", \" \")),\n",
    "                mo.md(_debug_text.replace(\"\\n\", \" \")),\n",
    "                mo.md(_new_text.replace(\"\\n\", \" \"))\n",
    "            ],\n",
    "            widths=[1, 1.2, 0.9]\n",
    "        ),\n",
    "        mo.hstack([\n",
    "            _table,\n",
    "            _table2\n",
    "        ], widths=[1, 5]),\n",
    "        mo.md(f\"Stop word ratio: {len([tok for tok in _doc if tok.is_stop]) / _doc.__len__()}\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"d-pre-cleanup\">d. Pre cleanup</h3>\n",
       "<span class=\"paragraph\">Find and replace certain tokens with regex (email, addresses, phone number, etc.)</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    pre_cleaned_df = load_dataset('pre_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    tqdm.pandas(desc=\"Running pre_cleanup\", ncols=100)\n",
    "\n",
    "    pre_cleaned_df = pd.DataFrame({\n",
    "        'text': deduplicated_df.progress_apply(\n",
    "            lambda x: pre_cleanup(x['subject'], x['text'], x['html']),\n",
    "            axis=1\n",
    "        ),\n",
    "        'label': deduplicated_df['label']\n",
    "    })\n",
    "\n",
    "    save_dataset(pre_cleaned_df, 'pre_cleaned')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### d. Pre cleanup\n",
    "\n",
    "Find and replace certain tokens with regex (email, addresses, phone number, etc.)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_cleanup(subject: str, text: str, html: str):\n",
    "    clean_subj = subject_cleanup(subject)\n",
    "    merged_text = merge_text_html(text, html)\n",
    "\n",
    "    pass_1 = f\"{clean_subj}\\n{merged_text}\"\n",
    "    pass_2 = artifact_cleanup(pass_1)\n",
    "\n",
    "    return pass_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text_html(text: str, html: str, threshold = 0.95) -> str:\n",
    "    if len(html) != 0:\n",
    "        html = html_cleanup(html)\n",
    "\n",
    "        # Some texts has leftover HTML somehow\n",
    "        text = html_cleanup(text)\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return html\n",
    "\n",
    "        else:\n",
    "            similarity = cosine_similarity(text, html)\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                return text\n",
    "\n",
    "            else:\n",
    "                return f\"{text}\\n{html}\"\n",
    "\n",
    "    else:\n",
    "        return html_cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_pattern = re.compile(\n",
    "#     r\"((http|ftp|https):\\/\\/)?([\\w_-]+(?:\\.[\\w_-]+)*\\.[a-zA-Z_-][\\w_-]+)([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    "# )\n",
    "\n",
    "# Remove mailing list information\n",
    "subject_cleanup_pattern = re2.compile(\n",
    "    r\"\\[.*?\\]\"\n",
    ")\n",
    "\n",
    "\n",
    "def artifact_cleanup(text):\n",
    "    artifact = \"--DeathToSpamDeathToSpamDeathToSpam--\"\n",
    "    return text.replace(artifact, \"\")\n",
    "\n",
    "\n",
    "def subject_cleanup(text: str) -> str:\n",
    "    return re2.sub(\n",
    "        pattern=subject_cleanup_pattern,\n",
    "        text=text,\n",
    "        repl=\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def html_cleanup(html):\n",
    "    if pd.isna(html):\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    # Artificially add _URL_ tokens\n",
    "    return soup.get_text(separator=\" \", strip=True) + \" google.com\" * len(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "name": "*cosine_similarity"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(s1: str, s2: str) -> float:\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert strings to character frequency vectors\n",
    "    vec1 = Counter(s1)\n",
    "    vec2 = Counter(s2)\n",
    "\n",
    "    # Calculating cosine similarity\n",
    "    dot_product = sum(vec1[ch] * vec2[ch] for ch in vec1)\n",
    "    magnitude1 = sqrt(sum(count ** 2 for count in vec1.values()))\n",
    "    magnitude2 = sqrt(sum(count ** 2 for count in vec2.values()))\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"e-post-cleanup\">e. Post cleanup</h3>\n",
       "<span class=\"paragraph\">Find and replace proper noun tokens (entities).</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    post_cleaned_df = load_dataset('post_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    _post_cleaned_emails = []\n",
    "    _truncated_texts = []\n",
    "    _skipped_idx = []\n",
    "\n",
    "    for i, row in pre_cleaned_df.iterrows():\n",
    "        if len(row['text']) > MAX_CHAR_LENGTH:\n",
    "            _skipped_idx.append(i)\n",
    "            _truncated_texts.append(row['text'][:MAX_CHAR_LENGTH])\n",
    "        else:\n",
    "            _truncated_texts.append(row['text'])\n",
    "\n",
    "    _post_cleanup_status = mo.status.progress_bar(\n",
    "        title=\"Running post_cleanup\",\n",
    "        total=len(pre_cleaned_df)\n",
    "    )\n",
    "\n",
    "    with _post_cleanup_status as _bar:\n",
    "        for _doc in process_batches(\n",
    "            nlp=nlp,\n",
    "            texts=_truncated_texts,\n",
    "            batch_size=5\n",
    "        ):\n",
    "            _post_cleaned_emails.append(nlp_pipeline(_doc))\n",
    "\n",
    "            _bar.update()\n",
    "\n",
    "    post_cleaned_df = pd.DataFrame({\n",
    "        'text': _post_cleaned_emails,\n",
    "        'label': pre_cleaned_df['label']\n",
    "    })\n",
    "\n",
    "    pre_cleaned_df[post_cleaned_df['text'] == \"\"]\n",
    "    post_cleaned_df = post_cleaned_df[post_cleaned_df['text'] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    save_dataset(post_cleaned_df, 'post_cleaned')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### e. Post cleanup\n",
    "\n",
    "Find and replace proper noun tokens (entities).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_CHAR_LENGTH = 15000\n",
    "\n",
    "\n",
    "def process_batches(nlp, texts, batch_size=None):\n",
    "    for i, doc in enumerate(nlp.pipe(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        disable=[\"parser\"]\n",
    "    )):\n",
    "        doc._.trf_data = None\n",
    "\n",
    "        yield doc\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            torch.cuda.empty_cache() # VRAM explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email matching\n",
    "email_pattern = re2.compile(\n",
    "    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    ")\n",
    "\n",
    "pgp_signature_pattern = re2.compile(\n",
    "    r\"(?s)-----BEGIN PGP SIGNATURE-----.*?-----END PGP SIGNATURE-----\"\n",
    ")\n",
    "\n",
    "\n",
    "def nlp_pipeline(doc: spacy.tokens.Doc, stop_threshold = 0.01, debug = False) -> str:\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in email_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"EMAIL\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in pgp_signature_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"PGP_SIG\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"DATE\", \"TIME\", \"MONEY\"]:\n",
    "                retokenizer.merge(ent)\n",
    "\n",
    "\n",
    "    # Treat as spam\n",
    "    stop_word_ratio = len([token for token in doc if token.is_stop]) / doc.__len__()\n",
    "    if stop_word_ratio < stop_threshold:\n",
    "        return \"\"\n",
    "\n",
    "    return \" \".join(\n",
    "        [token_processor(token, debug) for token in doc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "name": "*token_processor"
    }
   },
   "outputs": [],
   "source": [
    "def token_processor(token: spacy.tokens.Token, debug: bool = False) -> str:\n",
    "    def debug_fmt(tag: str) -> str:\n",
    "        spaced_text = \"~ ~\".join(token.text.split())\n",
    "        return f\"{tag} ~<-{spaced_text}~\"\n",
    "\n",
    "    if token.is_stop or token.tag_ == \"LS\":\n",
    "        return \"\" if not debug else f\"~~{token.text}~~\"\n",
    "\n",
    "    if token.like_url:\n",
    "        return \"_url_\" if not debug else debug_fmt(\"_url_\")\n",
    "\n",
    "    if token.like_email:\n",
    "        return \"_email_\" if not debug else debug_fmt(\"_email_\")\n",
    "\n",
    "    if token.ent_type_ and token.ent_type_ not in [\"CARDINAL\", \"ORDINAL\"]:\n",
    "        tag = f\"_{token.ent_type_}_\".lower()\n",
    "        return tag if not debug else debug_fmt(tag)\n",
    "\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        return \"_propn_\" if not debug else debug_fmt(\"_propn_\")\n",
    "\n",
    "    if token.pos_ == \"NUM\":\n",
    "        return \"_num_\" if not debug else debug_fmt(\"_num_\")\n",
    "\n",
    "    if token.pos_ == \"X\" and token.tag_ == \"FW\":\n",
    "        return \"_foreign_\" if not debug else debug_fmt(\"_foreign_\")\n",
    "\n",
    "    return token.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# II. Features engineering\n",
    "\n",
    "## 1. Additional features to consider\n",
    "\n",
    "- ~~Contains HTML (boolean)~~\n",
    "- Number of links (share the same token with links in text)\n",
    "- ~~Number of special characters~~ Might not be a good feature\n",
    "- Capitals ratio\n",
    "- ~~Email size~~ (maybe not)\n",
    "- Reply ratio, reply depth count\n",
    "- Stop word ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"a-capitals-ratio\">a. Capitals ratio</h3>\n",
       "<span class=\"paragraph\">I didn't use this yet, though.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_capitals_ratio(text: str) -> float:\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    capitals_count = sum(1 for c in text if c.isupper())\n",
    "    return capitals_count / len(text)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### a. Capitals ratio\n",
    "\n",
    "I didn't use this yet, though.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"b-number-of-special-characters\">b. Number of special characters</h3>\n",
       "<span class=\"paragraph\">On second thought..., I skipped it.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_special_chars(text):\n",
    "    special_chars = \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>?/`~\"\n",
    "    return sum(1 for char in text if char in special_chars)\n",
    "\n",
    "#df['text'].apply(count_special_chars)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### b. Number of special characters\n",
    "\n",
    "On second thought..., I skipped it.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"c-tf-idf\">c. TF-IDF</h3>\n",
       "<span class=\"paragraph\">Code copied from GeeksForGeeks:</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the data and transform it into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(post_cleaned_df['text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "### c. TF-IDF\n",
    "\n",
    "Code copied from GeeksForGeeks:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h1 id=\"iii-model-training\">III. Model training</h1>\n",
       "<h2 id=\"1-train-time\">1. Train time</h2>\n",
       "<span class=\"paragraph\">Here, we train our Logistic Regression model with 75-25 train-test split.</span></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = tfidf_df\n",
    "y = post_cleaned_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "# III. Model training\n",
    "\n",
    "## 1. Train time\n",
    "\n",
    "Here, we train our Logistic Regression model with 75-25 train-test split.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXTn",
   "metadata": {},
   "source": [
    "## Confusion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"2-confusion-time\">2. Confusion time</h2></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "mo.md(r\"\"\"\n",
    "## 2. Confusion time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Ham\", \"Spam\"]\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame({\n",
    "    'word': tfidf_df.columns,\n",
    "    'weight': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "\n",
    "top_spam = features_df.nlargest(20, 'weight')\n",
    "top_ham = features_df.nsmallest(20, 'weight')\n",
    "\n",
    "\n",
    "plot_df = pd.concat([\n",
    "    top_spam.assign(type='Spam'),\n",
    "    top_ham.assign(type='Ham')\n",
    "])\n",
    "\n",
    "plot_df['type'] = plot_df['weight'].apply(lambda x: 'Spam' if x > 0 else 'Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.hstack(\n",
    "    [\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_ham).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='greens', reverse=True, type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Ham words\",\n",
    "                height=600\n",
    "            )\n",
    "        ),\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_spam).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='reds', type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Spam words\",\n",
    "                height=600\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    justify='center',\n",
    "    widths=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {},
   "source": [
    "# Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.ui.dataframe(pre_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cleaned_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {},
   "source": [
    "## Beautiful Soup playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_input = mo.ui.text_area(placeholder=\"Paste HTML code...\", full_width=True)\n",
    "html_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_input.value, 'html5lib')\n",
    "cleaned_html = soup.get_text(separator=\" \", strip=True)\n",
    "print(cleaned_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {},
   "source": [
    "## RE2 playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = mo.ui.text_area(placeholder=\"Paste text...\", full_width=True)\n",
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_input = mo.ui.text(placeholder=\"Regex\", full_width=True)\n",
    "regex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgWD",
   "metadata": {},
   "outputs": [],
   "source": [
    "re2.sub(pattern=regex_input.value, text=text_input.value, repl=\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
