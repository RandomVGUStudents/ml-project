{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "marimo": {
     "name": "setup"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import some libs\n",
    "import altair as alt\n",
    "import glob\n",
    "import hashlib\n",
    "import mailparser\n",
    "import marimo as mo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import re2\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import tarfile\n",
    "import typing\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "from dateutil import parser\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Load NLP module\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "print(spacy.prefer_gpu())\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\"label\": \"TIME\", \"pattern\": [{\"SHAPE\": \"dd:dd:dd\"}]},\n",
    "]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Data acquisition & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Define dataset source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset_source = {\n",
    "    'easy_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\",\n",
    "        'count': 2500,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'easy_ham_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\",\n",
    "        'count': 1400,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'hard_ham': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\",\n",
    "        'count': 250,\n",
    "        'is_spam': False\n",
    "    },\n",
    "    'spam': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\",\n",
    "        'count': 500,\n",
    "        'is_spam': True\n",
    "    },\n",
    "    'spam_2': {\n",
    "        'url': \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\",\n",
    "        'count': 1397,\n",
    "        'is_spam': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"./datasets/\"\n",
    "\n",
    "\n",
    "def download_dataset(dataset_path: str, dataset_url: str):\n",
    "    tmp = wget.download(dataset_url)\n",
    "\n",
    "    with tarfile.open(tmp, \"r:bz2\") as tar:\n",
    "        tar.extractall(dataset_dir)\n",
    "\n",
    "    os.remove(os.path.join(dataset_path, \"cmds\"))\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Check dataset integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "\n",
    "for _dataset_name, _dataset_info in dataset_source.items():\n",
    "    _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "    if os.path.exists(_dataset_path):\n",
    "\n",
    "        for _filename in os.listdir(_dataset_path):\n",
    "            _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "            if os.path.isfile(_file_path):\n",
    "                _md5_hash = hashlib.md5(open(_file_path, 'rb').read()).hexdigest()\n",
    "                _provided_hash = _filename.split(\".\")[1]\n",
    "\n",
    "                # Ignore wrong hash from source\n",
    "                if (_md5_hash != _provided_hash and _provided_hash != \"244a63cd74c81123ef26129453e32c95\"):\n",
    "                    mo.md(f\"File {_filename} is corrupted, redownloading dataset {_dataset_name}...\")\n",
    "\n",
    "                    download_dataset(_dataset_name, _dataset_info['url'])\n",
    "                    os.remove(os.path.join(_dataset_path, _filename))\n",
    "\n",
    "                    break\n",
    "\n",
    "    else:\n",
    "        mo.md(f\"Dataset {_dataset_name} not found, downloading...\")\n",
    "        download_dataset(_dataset_path, _dataset_info['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Viewing some emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mails = glob.glob(\"./datasets/*/*\", recursive=True)\n",
    "idx = mo.ui.number(start=0, stop=len(mails) - 1, label=\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mail = mailparser.parse_from_file(mails[idx.value])\n",
    "\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "Path: `{mails[idx.value]}`\n",
    "\n",
    "**{mail.subject}**\n",
    "\n",
    "---\n",
    "\n",
    "{\"---\\n\\n---\".join(mail.text_plain)}\n",
    "\n",
    "---\n",
    "\n",
    "{mail.text_html}\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Load whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_checkpoints = {\n",
    "    'orig': {\n",
    "        'description': \"Dataset without preprocessing (6046 entries)\",\n",
    "        'checksum': \"6ab4e23a696aeac72ad3b38396666a25\"\n",
    "    },\n",
    "    'pre_cleaned': {\n",
    "        'description': \"Dataset after pre-cleanup (5851 entries)\",\n",
    "        'checksum': \"589f556bc02bdeaa376ca92a99e9755c\"\n",
    "    },\n",
    "    'post_cleaned': {\n",
    "        'description': \"Dataset after post-cleanup (5851 entries)\",\n",
    "        'checksum': \"d972cc17841c34ed3f73c1bd3e3fad62\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(checkpoint: str) -> pd.DataFrame:\n",
    "    dataset_path = os.path.join(dataset_dir, f\"{checkpoint}.gzip\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"No checkpoint '{checkpoint}' in {dataset_dir}\")\n",
    "\n",
    "    hash = hashlib.md5(open(dataset_path, 'rb').read()).hexdigest()\n",
    "    if hash != dataset_checkpoints[checkpoint]['checksum']:\n",
    "        raise ValueError(\"Data corrupted, or outdated. Check checksum.\")\n",
    "\n",
    "    data = pd.read_parquet(dataset_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = load_dataset('orig')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    data = {\n",
    "        'subject': [],\n",
    "        'text': [],\n",
    "        'html': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for _dataset_name, _dataset_info in dataset_source.items():\n",
    "        _dataset_path = os.path.join(dataset_dir, _dataset_name)\n",
    "\n",
    "        if os.path.exists(_dataset_path):\n",
    "\n",
    "            for _filename in os.listdir(_dataset_path):\n",
    "                _file_path = os.path.join(_dataset_path, _filename)\n",
    "\n",
    "                if os.path.isfile(_file_path):\n",
    "                    try:\n",
    "                        _mail = mailparser.parse_from_file(_file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with file {_file_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    data['subject'].append(_mail.subject)\n",
    "                    data['text'].append(\"\\n\".join(_mail.text_plain))\n",
    "                    data['html'].append(\"\\n\".join(_mail.text_html))\n",
    "                    data['label'].append(int(_dataset_info['is_spam']))\n",
    "\n",
    "    mo.md(f\"{str(e)}\\nLoading from files...\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    save_dataset(df, 'orig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def save_dataset(df: pd.DataFrame, checkpoint_name: str):\n",
    "    path = os.path.join(dataset_dir, f\"{checkpoint_name}.gzip\")\n",
    "    df.to_parquet(\n",
    "        path=path,\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    hash = hashlib.md5(open(path, 'rb').read()).hexdigest()\n",
    "    print(f\"File: {path}\\nMD5: {hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "hashlib.md5(open(\"./datasets/orig.gzip\", 'rb').read()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Preprocess email\n",
    "\n",
    "1. Remove duplicates from raw dataset\n",
    "2. Find and replace certain tokens with regex (email, addresses, phone number, etc.)\n",
    "3. Find and replace proper noun tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Data exploration notes\n",
    "\n",
    "- High correlation: HTML usage and spam\n",
    "- Dataset artifact: a lot of spam emails have this:\n",
    "\n",
    "```text\n",
    "--DeathToSpamDeathToSpamDeathToSpam--\n",
    "```\n",
    "\n",
    "- There might be (mostly in spam emails) forms with `_____` blanks.\n",
    "- Mailing list footer should be removed, they mostly appear in ham emails.\n",
    "\n",
    "```text\n",
    "-------------------------------------------------------\n",
    "This sf.net email is sponsored by:ThinkGeek\n",
    "Welcome to geek heaven.\n",
    "http://thinkgeek.com/sf\n",
    "_______________________________________________\n",
    "Spamassassin-talk mailing list\n",
    "Spamassassin-talk@lists.sourceforge.net\n",
    "https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
    "```\n",
    "\n",
    "- ~~Email replies appear with `> ` or `--] `, and \"On DATE, USER wrote:\"~~ Nvm, it's pretty random.\n",
    "- Mailling list (or some name) in subject that is not filtered (like ilug)\n",
    "- Entity labels exclusion: \"CARDINAL\", \"ORDINAL\"\n",
    "- Some emails are malformed, they are extremely long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = df.drop_duplicates(subset=['text', 'html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Preprocess preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='BYtC-0' random-id='0e751911-c6f3-1fac-b41f-7f7242afa5c9'><marimo-number data-initial-value='0' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert contents&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Number&lt;/span&gt;&lt;/span&gt;&quot;' data-start='0' data-stop='6045' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/W/1_school/WS2025/ML_Lab/.venv/lib/python3.13/site-packages/thinc/util.py:395: VisibleDeprecationWarning: This function is deprecated and will be removed in a future release. Use the cupy.from_dlpack() array constructor instead.\n",
      "  dlpack_tensor = xp_tensor.toDlpack()  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "_tfidf = TfidfVectorizer()\n",
    "_entry = deduplicated_df.iloc[idx.value]\n",
    "\n",
    "_clean_subj = subject_cleanup(_entry['subject'])\n",
    "_clean_html = html_cleanup(_entry['html'])\n",
    "_merged_text = merge_text_html(_entry['text'],_entry['html'])\n",
    "_text = f\"{_clean_subj}\\n{_merged_text}\"\n",
    "\n",
    "#_text_2 = random_cleanup(_text)\n",
    "_text_3 = artifact_cleanup(_text)\n",
    "\n",
    "_doc = nlp(_text_3)\n",
    "_newtext = nlp_pipeline(_doc)\n",
    "\n",
    "\n",
    "_table2 = mo.ui.table(\n",
    "    data=[{\n",
    "        'Token': token.text,\n",
    "        'Lemma': token.lemma_,\n",
    "        'PoS': token.pos_,\n",
    "        'Tag': token.tag_,\n",
    "        'Tag (explain)': spacy.explain(token.tag_),\n",
    "        'Entity': token.ent_type_,\n",
    "        'Stop word': token.is_stop,\n",
    "        'URL': token.like_url,\n",
    "        'Email': token.like_email\n",
    "    } for token in _doc],\n",
    "    pagination=True\n",
    ")\n",
    "\n",
    "_table3 = mo.ui.table(\n",
    "    data=[{\n",
    "        'Entity': ent.text,\n",
    "        'Label': ent.label_\n",
    "    } for ent in _doc.ents],\n",
    "    pagination=True\n",
    ")\n",
    "\n",
    "\n",
    "if len(_newtext) == 0:\n",
    "    _newtext = \"_<EMPTY>_\"\n",
    "\n",
    "_results = _tfidf.fit_transform([_newtext])\n",
    "_vocab = [{'word': _word, 'idx': _idx} for _word, _idx in _tfidf.vocabulary_.items()]\n",
    "_table = mo.ui.table(data=_vocab, pagination=True)\n",
    "\n",
    "\n",
    "mo.vstack(\n",
    "    [\n",
    "        mo.md(f\"Label: {\"Ham\" if _entry['label'] == 0 else \"Spam\"}\"),\n",
    "        mo.hstack(\n",
    "            [\n",
    "                mo.md(f\"{_clean_subj}\\n{_merged_text}\".replace(\"\\n\", \" \")),\n",
    "                mo.md(_text_3.replace(\"\\n\", \" \")),\n",
    "                mo.md(_newtext.replace(\"\\n\", \" \"))\n",
    "            ],\n",
    "            widths=\"equal\"\n",
    "        ),\n",
    "        mo.md(f\"Stop word ratio: {len([tok for tok in _doc if tok.is_stop]) / _doc.__len__()}\"),\n",
    "        mo.hstack([\n",
    "            _table,\n",
    "            _table2,\n",
    "            _table3\n",
    "        ])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Pre-cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = re.compile(\n",
    "    r\"((http|ftp|https):\\/\\/)?([\\w_-]+(?:\\.[\\w_-]+)*\\.[a-zA-Z_-][\\w_-]+)([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\"\n",
    ")\n",
    "\n",
    "# Remove mailing list information\n",
    "subject_cleanup_pattern = re2.compile(\n",
    "    r\"\\[.*?\\]\"\n",
    ")\n",
    "\n",
    "\n",
    "def artifact_cleanup(text):\n",
    "    artifact = \"--DeathToSpamDeathToSpamDeathToSpam--\"\n",
    "    return text.replace(artifact, \"\")\n",
    "\n",
    "\n",
    "def subject_cleanup(text: str) -> str:\n",
    "    return re2.sub(\n",
    "        pattern=subject_cleanup_pattern,\n",
    "        text=text,\n",
    "        repl=\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def html_cleanup(html):\n",
    "    if pd.isna(html):\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    # Artificially add _URL_ tokens\n",
    "    return soup.get_text(separator=\" \", strip=True) + \" google.com\" * len(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     },
     "name": "*cosine_similarity"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(s1: str, s2: str) -> float:\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert strings to character frequency vectors\n",
    "    vec1 = Counter(s1)\n",
    "    vec2 = Counter(s2)\n",
    "\n",
    "    # Calculating cosine similarity\n",
    "    dot_product = sum(vec1[ch] * vec2[ch] for ch in vec1)\n",
    "    magnitude1 = sqrt(sum(count ** 2 for count in vec1.values()))\n",
    "    magnitude2 = sqrt(sum(count ** 2 for count in vec2.values()))\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text_html(text: str, html: str, threshold = 0.95) -> str:\n",
    "    if len(html) != 0:\n",
    "        html = html_cleanup(html)\n",
    "\n",
    "        # Some texts has leftover HTML somehow\n",
    "        text = html_cleanup(text)\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return html\n",
    "\n",
    "        else:\n",
    "            similarity = cosine_similarity(text, html)\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                return text\n",
    "\n",
    "            else:\n",
    "                return f\"{text}\\n{html}\"\n",
    "\n",
    "    else:\n",
    "        return html_cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def pre_cleanup(subject: str, text: str, html: str):\n",
    "    clean_subj = subject_cleanup(subject)\n",
    "    merged_text = merge_text_html(text, html)\n",
    "\n",
    "    pass_1 = f\"{clean_subj}\\n{merged_text}\"\n",
    "    pass_2 = artifact_cleanup(pass_1)\n",
    "\n",
    "    return pass_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    pre_cleaned_df = load_dataset('pre_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    tqdm.pandas(desc=\"Running pre_cleanup\", ncols=100)\n",
    "\n",
    "    pre_cleaned_df = pd.DataFrame({\n",
    "        'text': deduplicated_df.progress_apply(\n",
    "            lambda x: pre_cleanup(x['subject'], x['text'], x['html']),\n",
    "            axis=1\n",
    "        ),\n",
    "        'label': deduplicated_df['label']\n",
    "    })\n",
    "\n",
    "    save_dataset(pre_cleaned_df, 'pre_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Post cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email matching\n",
    "email_pattern = re2.compile(\n",
    "    r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    ")\n",
    "\n",
    "pgp_signature_pattern = re2.compile(\n",
    "    r\"(?s)-----BEGIN PGP SIGNATURE-----.*?-----END PGP SIGNATURE-----\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "MAX_CHAR_LENGTH = 15000\n",
    "\n",
    "\n",
    "def process_batches(nlp, texts, batch_size=None):\n",
    "    for i, doc in enumerate(nlp.pipe(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        disable=[\"parser\"]\n",
    "    )):\n",
    "        doc._.trf_data = None\n",
    "\n",
    "        yield doc\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        #    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #raise(ValueError(\"Code changed. Rebuilding dataset...\"))\n",
    "    post_cleaned_df = load_dataset('post_cleaned')\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    _post_cleaned_emails = []\n",
    "    _truncated_texts = []\n",
    "    _skipped_idx = []\n",
    "\n",
    "    for i, row in pre_cleaned_df.iterrows():\n",
    "        if len(row['text']) > MAX_CHAR_LENGTH:\n",
    "            _skipped_idx.append(i)\n",
    "            _truncated_texts.append(row['text'][:MAX_CHAR_LENGTH])\n",
    "        else:\n",
    "            _truncated_texts.append(row['text'])\n",
    "\n",
    "    _post_cleanup_status = mo.status.progress_bar(\n",
    "        title=\"Running post_cleanup\",\n",
    "        total=len(pre_cleaned_df)\n",
    "    )\n",
    "\n",
    "    with _post_cleanup_status as _bar:\n",
    "        for _doc in process_batches(\n",
    "            nlp=nlp,\n",
    "            texts=_truncated_texts,\n",
    "            batch_size=5\n",
    "        ):\n",
    "            _post_cleaned_emails.append(nlp_pipeline(_doc))\n",
    "\n",
    "            _bar.update()\n",
    "\n",
    "    post_cleaned_df = pd.DataFrame({\n",
    "        'text': _post_cleaned_emails,\n",
    "        'label': pre_cleaned_df['label']\n",
    "    })\n",
    "\n",
    "    pre_cleaned_df[post_cleaned_df['text'] == \"\"]\n",
    "    post_cleaned_df = post_cleaned_df[post_cleaned_df['text'] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    save_dataset(post_cleaned_df, 'post_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     },
     "name": "*token_processor"
    }
   },
   "outputs": [],
   "source": [
    "def token_processor(token: spacy.tokens.Token) -> str:\n",
    "    if token.is_stop:\n",
    "        return \"\"\n",
    "\n",
    "    if token.like_url:\n",
    "        return \"_url_\"\n",
    "    if token.like_email:\n",
    "        return \"_email_\"\n",
    "\n",
    "    if token.ent_type_ not in [\"\", \"CARDINAL\", \"ORDINAL\"]:\n",
    "        return f\"_{token.ent_type_}_\".lower()\n",
    "\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        return \"_propn_\"\n",
    "    if token.pos_ == \"NUM\":\n",
    "        return \"_num_\"\n",
    "\n",
    "    if token.tag_ == \"LS\":\n",
    "        return \"\"\n",
    "    if (token.pos_ == \"X\") and (token.tag_ == \"FW\"):\n",
    "        return \"_foreign_\"\n",
    "\n",
    "    else:\n",
    "        return token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(doc: spacy.tokens.Doc, stop_threshold = 0.01) -> str:\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in email_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"EMAIL\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match in pgp_signature_pattern.finditer(doc.text):\n",
    "            span = doc.char_span(\n",
    "                match.start(),\n",
    "                match.end(),\n",
    "                alignment_mode='expand'\n",
    "            )\n",
    "\n",
    "            if span is not None:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token.ent_type_ = \"PGP_SIG\"\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"DATE\", \"TIME\", \"MONEY\"]:\n",
    "                retokenizer.merge(ent)\n",
    "\n",
    "\n",
    "    # Treat as spam\n",
    "    stop_word_ratio = len([token for token in doc if token.is_stop]) / doc.__len__()\n",
    "    if stop_word_ratio < stop_threshold:\n",
    "        return \"\"\n",
    "\n",
    "    return \" \".join(\n",
    "        [token_processor(token) for token in doc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kqZH",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Additional features to consider\n",
    "\n",
    "- ~~Contains HTML (boolean)~~\n",
    "- Number of links (share the same token with links in text)\n",
    "- ~~Number of special characters~~ Might not be a good feature\n",
    "- Capitals ratio\n",
    "- ~~Email size~~ (maybe not)\n",
    "- Reply ratio, reply depth count\n",
    "- Stop word ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Capitals ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {
    "marimo": {
     "name": "*calc_capitals_ratio"
    }
   },
   "outputs": [],
   "source": [
    "def calc_capitals_ratio(text: str) -> float:\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    capitals_count = sum(1 for c in text if c.isupper())\n",
    "    return capitals_count / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SdmI",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Number of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgWD",
   "metadata": {
    "marimo": {
     "name": "*count_special_chars"
    }
   },
   "outputs": [],
   "source": [
    "def count_special_chars(text):\n",
    "    special_chars = \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>?/`~\"\n",
    "    return sum(1 for char in text if char in special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yOPj",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].apply(count_special_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fwwy",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LJZf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the data and transform it into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(post_cleaned_df['text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urSm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxvo",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df\n",
    "y = post_cleaned_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mWxS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Confusion time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CcZR",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWSi",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Ham\", \"Spam\"]\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlud",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZnO",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame({\n",
    "    'word': tfidf_df.columns,\n",
    "    'weight': logreg.coef_[0]\n",
    "})\n",
    "\n",
    "\n",
    "top_spam = features_df.nlargest(20, 'weight')\n",
    "top_ham = features_df.nsmallest(20, 'weight')\n",
    "\n",
    "\n",
    "plot_df = pd.concat([\n",
    "    top_spam.assign(type='Spam'),\n",
    "    top_ham.assign(type='Ham')\n",
    "])\n",
    "\n",
    "plot_df['type'] = plot_df['weight'].apply(lambda x: 'Spam' if x > 0 else 'Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xvXZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.hstack(\n",
    "    [\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_ham).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='greens', reverse=True, type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Ham words\",\n",
    "                height=600\n",
    "            )\n",
    "        ),\n",
    "        mo.ui.altair_chart(\n",
    "            alt.Chart(top_spam).mark_bar().encode(\n",
    "                x='weight:Q',\n",
    "                y=alt.Y('word:N', sort='null'),\n",
    "                color=alt.Color('weight:Q', scale=alt.Scale(scheme='reds', type='log')),\n",
    "                tooltip=['word', 'weight']\n",
    "            ).properties(\n",
    "                title=\"Spam words\",\n",
    "                height=600\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    justify='center',\n",
    "    widths=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CLip",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YECM",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='YECM-0' random-id='e90c71ef-ba10-34f4-ee72-e959ae57e920'><marimo-dataframe data-initial-value='{&quot;transforms&quot;:[]}' data-label='null' data-columns='[[&quot;text&quot;,&quot;string&quot;,&quot;object&quot;],[&quot;label&quot;,&quot;integer&quot;,&quot;int64&quot;]]' data-dataframe-name='&quot;pre_cleaned_df&quot;' data-total='5851' data-page-size='5' data-show-download='true'></marimo-dataframe></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.ui.dataframe(pre_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEAS",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cleaned_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXej",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Beautiful Soup playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJmg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='EJmg-0' random-id='6a40fc64-c3e4-2196-f3b2-500f1947daad'><marimo-text-area data-initial-value='&quot;&quot;' data-label='null' data-placeholder='&quot;Paste HTML code...&quot;' data-disabled='false' data-debounce='true' data-full-width='true'></marimo-text-area></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_input = mo.ui.text_area(placeholder=\"Paste HTML code...\", full_width=True)\n",
    "html_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UmEG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viagra Online Now!!                         22560 VIAGRA the breakthrough medication for impotence delivered to your mailbox... ..without leaving your computer. Simply Click Here: http://host.1bulk-email-software.com/ch4/pharm/blue In less than 5 minutes you can complete the on-line consultation and in many cases have the medication in 24 Â hours. Simply Click Here: http://host.1bulk-email-software.com/ch4/pharm/blue >From our website to your mailbox. On-line consultation for treatment of compromised sexual function. Convenient...affordable....confidential. We ship VIAGRA worldwide at US prices. To Order Visit: http://host.1bulk-email-software.com/ch4/pharm/blue This is not a SPAM. You are receiving this because you are on a list of email addresses that I have bought. And you have opted to receive information about business opportunities. If you did not opt in to receive information on business opportunities then please accept our apology. To be REMOVED from this list simply reply with REMOVE as the subject. And you will NEVER receive another email from me. mailto: remove432@businessinfo-center.com\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_input.value, 'html5lib')\n",
    "cleaned_html = soup.get_text(separator=\" \", strip=True)\n",
    "print(cleaned_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vEBW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## RE2 playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kLmu",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = mo.ui.text_area(placeholder=\"Paste text...\", full_width=True)\n",
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IpqN",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_input = mo.ui.text(placeholder=\"Regex\", full_width=True)\n",
    "regex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxZZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "re2.sub(pattern=regex_input.value, text=text_input.value, repl=\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
